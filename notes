
eig matrix
----------

For an n leaf tree, we have dim(A)=n-1 for the tree algebra.

Let s(L_i)  be the ith (ordered/include multiplicity) spectrum of the basis matrix L_i. So s(L_i) is an n-vector.

Each L_i has a 0 as an eigenvalue corresponding to the col vector of all 1s. Remove this from the spectrum vectors.

Thus we have (n-1) (n-1)-vectors.

Form a square matrix out of these and take the absolute value of its determinant.

Then this value will only depend on the tree shape, since the determinant simply acquires +/- under permutation of rows/columns.

(Geometrically we can consider this to be the volume spanned by the spectrum vectors.)



basis variance
--------------

if detsubst(L*) = the determinant as described, but with respect to basis L*, L_2, L_3... L_n-1, so that detsubst(L_1) is exactly the determinant as described

then detsubst(L_1 + L_2) expanded on the first column seems to give the same expression as detsubst(L_1) + detsubst(L_2) expanded on their first columns

but detsubst(L_2) is the determinant of a singular matrix [spec(L_2), spec(L_2), spec(L_3)...], so equals zero

the same doesn't seem to apply to detsubst(L_1 + L_1) though, so we might need to normalize the L_i matrices somehow?



degenerecy and eig matrix
-------------------------

If two subtrees of the tree have the same spectrum (e.g. if they are the same)
then the matrix will have a repeated row, and hence will not be invertible, and
hence will have a determinent of zero, however this is specifically the case
where the vector space generated by the spectra is no longer trivial.



symmetry and degeneracy
-----------------------

when taking the span or determinent of the spectrum vectors, we see that
repeated spectra give a trivial span of C^(n-1), and a determinent of 0

As such we care about whether a tree has degeneracy or not, and it would seem
that a tree with symmetry would be degenerate, e.g.:

   ^
  / \
 ^   ^
1 2 3 4

it looks like the 2nd and 3rd internal node have the same spectrum, being
relabellings of eachother, but in fact they do not, since we are looking at the
spectra generated by _a fixed sequence of eigenvectors_, and hence relabelling
the leaves results in the eigenvectors being permuted, which is undone only by
permuting the eigenvalues, giving a different spectrum.

A = PBP', B = VLV'
A = (PV)L(PV)'
so A v_p(i) = l_i v_p(i)



entropy and computation
-----------------------

The matrix representations simply correspond {i_l} x {i_r}
the leaf descendents of the left child of the node,
multiplied by the leaf descendents of the right child of the node
if we consider these matrices invariant under relabelling (which they aren't
quite) then it looks like this is just representing the size of each child
tree, and we could canonicalize as such:

any subtree's matrix is similar by permutation to a matrix of the form:
m_ij = -y if i = j <= x
     = -x if x < i = j <= x + y
     = 1 if i <= x < j <= x + y
     = 1 if j <= x < i <= x + y
     = 0 otherwise

where x is the number of leaves on the left of the subtree's root,
and y is the number of leaves on the right
x + y = n_i <= n

this could be very useful computationally, especially since these matrices can
be extended or retracted by the rows and columns of zero

such an extension simply adds a zero eigenvalue corresponding to
[0, .. 0, 1]
and a zero coordinate to each existing eigenvector without changing the
eigenvalue

this means that each spectrum will only have x + y - 1 non-zero eigenvalues,
so the matrix of eigenvalues will be about half zeroes

in fact for the case of the balanced 4-tree, the nodes and eigenvectors can be
rearranged so that the matrix has the form:
[ a 0 b ]
[ 0 c d ]
[ 0 0 e ]

which will clearly have a determinant of a*c*e

it is not at all clear whether these upper triangular forms exist for all
trees, since the eigenvectors don't clearly correspond to leaves of the tree,
(if they did we might want to order the trees from smallest to largest, or
order the leaves of the tree by the number of subtrees they sit inside)
in fact eigenvectors in the null space may be an arbitrary basis of the null
space, however computing the eigenvalue matrices of more trees may provide
insight



closed form eigenvectors
------------------------

given the simple M_ij form above, it becomes possible to find eigenvectors for
these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

[-1  0  1][1  1  1]   [1  1  1][0  0  0]
[ 0 -1  1][1 -1  1] = [1 -1  1][0 -1  0]
[ 1  1 -2][1  0 -2]   [1  0 -2][0  0 -3]

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

[-1  0  1][ 1]     [ 1]
[ 0 -1  1][-1] = -1[-1]
[ 1  1 -2][ 0]     [ 0]

in the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
in the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of <1, -1, 0, .. 0> will generalize to any M_ij as above, as long as
x >= 2, with an eigenvalue of -y
similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of <0, .. 0, -1, 1> with an eigenvalue of -x
further still we can get x-1 of the former and y-1 of the latter by moving the
-1 coordinate to any other row <= x, and the above argument would still apply.

this gives x-1 repeated eigenvalues of -y, y-1 repeated eigenvalues of -x,
which when combined with the 0 eigenvalue shown, and the n - (x + y) trivial 0
eigenvalues coming from rows that are all zero, we get n-1 total eigenvalues,
meaning there is 1 more.

This must be the third eigenvector we get in the simple 3-tree case:

[-1  0  1][ 1]     [ 1]
[ 0 -1  1][ 1] = -3[ 1]
[ 1  1 -2][-2]     [-2]

it's hard to understand exactly why this is an eigenvector, but if we guess
that the general form is <a, a, .. a, b, b, .. b> so that the a repeats y
times, and the b repeats x times, then upon application of M as above, we get

if i <= x
(Mv)ij = -y*a + y*b
if i > x
(Mv)ij = x*a - x*b

then if we suppose that these equal l*a and l*b respectively, we get:

(yb - ay)/a = (ax - xb)/b
b^2y - aby = a^2x - abx
b^2y + ab(x - y) - a^2x = 0
let r = b/a
r = [ -(x-y) +/- sqrt((x-y)^2 + 4xy) ] / 2y
  = [ y - x +/- (x + y) ] / 2y
  = 1 or -x/y

r = 1 corresponds to our common eigenvector from above, so it is not new.
r = -x/y is new however, so set b = -x, a = y,

then our eigenvalue l
 = y(b - a)/a
 = -(x + y)

This is our last eigenvalue, corresponding to the eigenvector
<y, y, .. y, -x, -x, .. -x> where y is repeated x times, and x is repeated y
times.


Simultaneous Diagonalization
----------------------------

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices.

If we look at each matrix L_i, we can observe that the last eigenvalue in our
list above, l = -(x + y) is unique, and hence the eigenvector that corresponds
to it is uniquely determined by it. (modulo scaling)

Since each matrix will give a different, uniquely determined vector for its
most unique eigenvalue, we have n-1 eigenvectors, which are also distinct from
our other uniquely determined vector <1, 1, .. 1>.

So we know that that all of our eigenvectors are determined, and can use this
set of eigenvectors for diagonalization of any basis matrix.


If we number these vectors v_i, and set v_0 to be <1, 1, .. 1>, then we can
understand which eigenvalues will correspond to different eigenvectors.

For L_i, v_i will by definition give L_i v_i = -(x+y) v_i

for nodes descending from the left child of i, L_i v_l = -y v_l
and similarly on the right, L_i v_r = -x v_r
this can be seen both by the quantities of nodes, and more rigorously by the
fact that each of these v_l is a linear combination of vectors constructed
above, with eigenvalue -y, and similarly for v_r and -x

for nodes above and anywhere else, the nontrivial part of L_i v_j will be
entirely within one of the four consistent sections of v_j, so v_j might as
well be a multiple of v_0, giving L_i v_j = 0 v_j

So we know all n of our eigenvalues, and all of their corresponding
eigenvectors.


Returning to the determinant
----------------------------

If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular, and so its determinant will
simply be

pi i = 1, n, d_i

this is simply the pre-order of the nodes, and so we have a closed form of the
determinant.

the reference paper "ubiquity of synonymity" used an induction-like argument,
that if two trees are synonymous, then adding branches around those trees will
create further pairs of synonymous trees, which looks almost trivial to show
for this spectral determinant.

As such if there is any synonymity then as the paper showed, it will become
ubiquitous for large enough n.

It seems unlikely that there would be _no_ synonymity, which is the only way
for there to be _anything less than ubiquitous_ synonymity.
