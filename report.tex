\documentclass{report}
\usepackage{amsmath, amssymb, amsthm, dsfont, qtree}
\begin{document}

\newtheorem{definition}{Definition}

\section{Abstract}

We investigate representations of binary trees related to a particular matrix
algebra representation demonstrated previously.

[finish report first then finish abstract]

\section{Tree Construction}

Trees will be represented as directed graphs, that is a set of ordered pairs.

\begin{definition} Binary Tree with $n$ leaves

	A graph is binary tree with $n$ leaves if and only if
	\begin{itemize}
		\item there are no cycles (nontrivial paths with equal endpoints)
		\item every node has exactly one predecessor, except one node called the root, which has no predecessor
		\item every node has either 0 children (a leaf) or 2 children (a branch)
		\item there are exactly $n$ leaves
	\end{itemize}
\end{definition}

For convenience we will also assume that the tree is labelled with leaves $1$ through to $n-1$,
hence leaves are labelled $n$ through to $2n-1$.

Hence define $B$ to be the set of branch labels, $\{b | b \in \mathds{Z}, 1 \leq b < n\}$.
Similarly $L$ to be the leaf labels, $\{l | l \in \mathds{Z}, n \leq l < 2n-1\}$.

\subsection{Most Recent Common Ancestor}

\begin{definition} Ancestry

	If a node r is the root of a tree, then its ancestry is simply the singleton ordered list $(r)$.

	If a node r is not the root of a tree, then its ancestry is acquired by taking the ancestry of its predecessor, and prepending r itself.

	In other words the ancestry of a node is the list of nodes above that node, starting with itself, and ending with the root of the tree.
\end{definition}

\begin{definition} Common Ancestry

	The common ancestry of two nodes is the largest ordered list of nodes that is a suffix of both nodes' ancestries.
\end{definition}

\begin{definition} Most Recent Common Ancestor

	The MRCA of two nodes (in particular two leaves) is the first node in the common ancestry of the two nodes.
\end{definition}

% is it ok to take graph concepts like predecessor for granted
% also self explanatory concepts used later such as descendants, children, descendant leaves, etc.

\subsection{Relabelling}

Trees can be relabelled by injective functions on their node labels.

\DeclareMathOperator{\relabel}{relabel}

\begin{definition} Relabel

	We can use a permutation $\sigma \in S_{2n-1}$ to relabel a tree by taking the group action of $S_{2n-1}$ on a graph.

	\[
		\relabel(\sigma, G) := \{(\sigma(i), \sigma(j)) | (i, j) \in G\}
	\]
\end{definition}

Just as group actions give us a concept of relabelling, the orbits of these actions will give a set of trees that are relabellings of eachother.

This can give us an exhaustive set of trees with the same structure, which we can take to represent the structure itself:

\begin{definition} Tree Structure

	The structure of a tree with $n$ leaves, is simply the orbit of that tree under the $\relabel$ action of the group $S_BS_L$.

	Similarly the group $S_B$ could be thought of as that tree without branch labels, and $S_L$ as that tree without leaf labels.
\end{definition}

\begin{definition} Invariance Under Relabelling

	A function or operation on binary trees with n leaves, is invariant under relabelling, if and only if trees with the same structure map to the same result.

	Similarly a function or operation is invariant under branch/leaf relabellings if trees that are equal with their branches/leaves removed, map to the same result.

	These properties could also be invoked "up to isomorphism" or up to some other class of operations.

	This simply means that the results of the function don't need to be exactly the same, they just need to be mapped to eachother by some operation in the specified class.
\end{definition}

\subsection{Hierarchy}

There is another way of representing a tree without its branch labels, which is by representing it as a hierarchy based on sets of leaves under a node.

[briefly define hierarchies and maybe argue for their equivalence to trees without branch labels]

\section{Constructing Measurements of the Algebra}

\subsection{Background}

The paper ``Ubiquity of synonymity'' dealth with 3 different matrix
representations of trees: the adjacency matrix, laplacian matrix, and distance
matrix.
These representations are interesting, since relabellings of the tree result
in similar matrices, so their spectra will not change.
This means the spectra are themselves a representation of the tree, but the
paper shows that these representations are not fair, and in fact `approach'
triviality as the number of leaves increases.

A motivation indicated by the paper is to derive a distance function between
the structure of two trees.
Such a function would still have potential use if it was derived from an unfair
representation, but the result that representations become near trivial means
most trees would also have zero distance
As such while it would be nice to have a fair representation, it is crucial
that it is at least substantially non-trivial.

The Tas Phylo group has shown in another paper that trees can be represented as
an algebra of matrices, and that this algebra is commutative.

The algebra is as follows:

\begin{definition} Algebra of a Tree

	Given a binary tree with $n$ leaves, that tree's algebra is an $n-1$-dimensional commutative algebra spanned by $n-1$ basis matrices corresponding to the $n-1$ branch nodes of the tree.

	The basis matrix $r$ in a tree with $n$ leaves has the following definition:
	\[ L_{ij} = \begin{cases}
		1 & i \neq j \and MRCA(i, j) = r
		0 & i \neq j \and MRCA(i, j) \neq r
		-\sum_{k \neq j} L_{ik} & otherwise
	\end{cases} \]
\end{definition}

% proper references?

Similar to the previous paper, these matrices relate rows/columns to leaf
labels, and hence are similar to matrices associated with relabellings of the
tree.
Further, since the algebra is commutative, we have the property that if it
diagonalizable at all, then it is simultaneously diagonalizable.
Together these properties suggest that the spectra of matrices in this algebra
might be useful.

\subsection{Canonical Forms of Spectra}

If we assume that the matrices in the algebra are diagonalizable for now, (we
can show this later) then we can start to reason about the spectra of these
matrices.
With this assumption, it will also follow that all of the matrices in the
algebra are simultaneously diagonalizable, and hence that a set of eigenvectors
can be found common to the whole algebra.
We shall therefore refer to these as the Algebra's eigenvectors.

With this in place, we consider the spectra of the matrices in the algebra.
When representing each tree as a single matrix, comparing the spectra of these
matrices was a simple matter of comparing the multiplicity of each eigenvalue.
This could be thought of as either comparing the spectrum as a multiset, or as
a sorted sequence of values.
Then since the order of the eigenvectors doesn't matter, we can freely relabel
trees without changing their representation.
%     I keep changing my wording, I should try to stick to the original wording
% of the reference papers. "invariance under relabelling" etc.

We, on the other hand, are considering a multidimensional space of matrices,
and it might be nice to have this same property of invariance under
relabelling.
As such we might consider the set of sorted spectra of the whole algebra.

This representation seems to be useful, but once we consider the simultaneous
diagonalization of the algebra, we see a missed opportunity to generate a
homomorphism from the algebra to the vector space $\mathds{C}^n$.
That is, by taking the algebra's eigenvectors under some ordering, and
generating the spectra of matrices corresponding to these eigenvectors, we will
get the property that the spectrum of a linear combination of matrices is the
same linear combination of the individual spectra.
This means the ``spectrum'' map is a homomorphism, and so its image will be a
vector space.

This will not apply to the sorted image, e.g. $<1, 1> - <0, 1> = <1, 0>$
	% <tuple> notation?
So as an alternative, we could consider this vector space, and look for
properties that are invariant under relabelling of the original tree, and
invariant under different choices of eigenvector.
This is the same as finding properties that are invariant under permutation of
the axes of the vector space.

% note that it might look like we have strayed from any sense of distance formula
% measurements later on will have potential for distance formulae

\subsection{Subspace Triviality}

When looking at this vector space, the most obvious measurement we could apply
to it is its dimension.
It will turn out that its dimension is always the same as the algebra, which
seems odd when the algebra seems to contain matrices that are just relabellings
of eachother.
As an example every cherry in a tree will correspond to a matrix equivalent to
the following:

\[
	\left[ \begin{matrix}
		-1 & 1\\
		1 & -1
	\end{matrix} \right]
\]

The exact matrices are simply the above with added rows and columns of zero.
The spectrum of any matrix in this form should be the same as any other, and in
the sorted-list representation they will be, but our vector space will
distinguish these, in the same way that $<0, 1>$ and $<1, 0>$ are linearly
independent.

This means that symmetries in the tree might have created interesting
redundancies in the set of sorted spectra, but our vector space will not
reflect these symmetries in the same way.

In particular if we say that the number of leaves on the original tree is $n$,
then our algebra will be generated from $n-1$ internal nodes, and will be a set
of $n \times n$ matrices.

This means that the vector space will be an $n-1$ dimensional subspace of
$\mathds{C}^n$.

Unfortunately since the matrices all have zero row-sum, that corresponds to an
eigenvector of all 1s, and an eigenvalue of zero, which means the subspace will
simply be $\mathds{C}^{n-1}$ extended by a zero, regardless of the tree in
question.

Not only is the vector space not a fair representation of the binary tree, it
is a trivial representation.

\subsection{Parameter-Free Representations}

While it is not useful to map the whole algebra to $\mathds{C}^n$, if we just
look at the basis it turns out to be more useful.
% addressing the fact that other bases exist than the one the algebra was
% defined with? once we're talking about determinants this can be dealt with
% specifically, but the trace and spectrum of the eigenvalue matrix are heavily
% dependent on the basis chosen
As such we shall consider the basis that was used to define the algebra.

If the original tree has $n$ leaves, then we have
\begin{itemize}
	\item $n-1$ internal nodes
	\item $n-1$ basis matrices
	\item $n$ common eigenvectors
	\item $(n-1) \times n$ eigenvalues
\end{itemize}

This could be summarized in a pair of matrices:
\begin{itemize}
	\item An $n \times n$ matrix of eigenvectors, and
	\item an $(n-1) \times n$ matrix of eigenvalues.
\end{itemize}

This second matrix will always have a column of zeroes in it, corresponding to
the eigenvector of all 1s, since the algebra has zero row-sum.
As such we do not lose any information by removing this column, and getting a
square $(n-1) \times (n-1)$ matrix.
We could do the same for the eigenvector and get an $n \times (n-1)$ matrix,
but obviously this has the opposite effect.

These matrices are not invariant under relabelling:
\begin{itemize}
	\item relabelling leaves permutes the rows of the eigenvector matrix
	\item relabelling internal nodes permutes the rows of the eigenvalue matrix
	\item relabelling eigenvectors permutes the columns of both matrices.
\end{itemize}

It seems like either of these matrices on their own would be enough to
reconstruct the algebra, and hence the tree, so on their own they are
interesting representations of a tree.
Further still, measurements that are invariant under the row/column
permutations above have potential for distinguishing tree-shapes as was
originally desired.

One clear possibility is to take the determinant of either of these matrices,
and remove the effect of row/column permutations with an absolute value.
For now we shall derive the exact value of both of the matrices.

% not included currently, could be a useful analogy:
% Geometrically we can consider this to be the volume spanned by the spectrum vectors.

\section{Simultaneous Diagonalization}

%\subsection{basis variance}
%
%If $detsubst(L*) =$ the determinant as described, but with respect to basis
%$L*, L_2, L_3\ldots L_n-1$, so that $detsubst(L_1)$ is exactly the
%determinant as described.
%
%Then $detsubst(L_1 + L_2)$ expanded on the first column seems to give the same
%expression as $detsubst(L_1) + detsubst(L_2)$ expanded on their first columns.
%
%But $detsubst(L_2)$ is the determinant of a singular matrix:
%\[[spec(L_2), spec(L_2), spec(L_3)\ldots]\]
%so equals zero.
%
%The same doesn't seem to apply to $detsubst(L_1 + L_1)$ though, so we might
%need to normalize the $L_i$ matrices somehow?

When working towards a simultaneous diagonalization of the algebra, the first
thing we need is to show that the matrices can be diagonalized at all.
In showing this we expect to get the eigenvalues as well.


\subsection{Simple Matrix Format}

For simplicity we will assume that the tree leaves have been labelled ``from
left to right'', i.e.\ such that at any internal node, its left descendants are
less than its right descendants.

Formally, that there is a total order of the tree such that the ``left'' child
of any node (being the minimum of the two children) has descendents that are
all less than the ``right'' child of that node.

Other labellings will simply permute the coordinates of the eigenvectors we
derive here.

Then we can see that the matrix that corresponds to a node with $x$ left
descendents, $y$ right descendents, and minimum descendent $l+1$. (so that
there are $l$ leaves to the left of the node's minimum descendent)

\[ M_{ij} = \begin{cases}
-y & l < i = j \leq l + x\\
-x & l + x < i = j \leq l + x + y\\
1 & l < i \leq l + x < j \leq l + x + y\\
1 & l < j \leq l + x < i \leq l + x + y\\
0 & otherwise
\end{cases} \]

So for example if we take the tree:

\Tree[. 1 [. [.i [. 2 3 ] 4 ] 5 ]]

Then the node marked $i$ will have
$l=1$, $x=2$, $y=1$
which gives
\[ L_i = M = \left[ \begin{matrix}
	0 & 0 & 0 & 0 & 0\\
	0 & -1 & 0 & 1 & 0\\
	0 & 0 & -1 & 1 & 0\\
	0 & 1 & 1 & -2 & 0\\
	0 & 0 & 0 & 0 & 0
\end{matrix} \right] \]

From this it becomes clear that all but $x+y$ of the eigenvalues will be zero,
and zero-rowsum brings this down to $x+y-1$ non-zero eigenvalues.

As a side note this means that the matrix of all of the eigenvalues will be
at least half zeroes, which simplifies a lot of calculations on this
matrix, especially once we find a way of permuting the eigenvalue matrix to be
upper triangular.

\subsection{Diagonalization}

given the simple $M_{ij}$ form above, it becomes possible to find eigenvectors
for these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
=
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
\left[\begin{matrix}
	0 & 0 & 0\\
	0 & -1 & 0\\
	0 & 0 & -3
\end{matrix}\right]
\end{equation*}

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
=
-1
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
\end{equation*}

In the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
In the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of $<1, -1, 0\ldots 0>$ will generalize to any $M_{ij}$ as above, as
long as $x \geq 2$, with an eigenvalue of -y.
Similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of $<0\ldots 0, -1, 1>$ with an eigenvalue of
-x.

Further still, we can get $x-1$ of the former and $y-1$ of the latter by moving
the -1 coordinate to any other row $\leq$ x, and the above argument would still
apply.

This gives $x-1$ repeated eigenvalues of $-y$, $y-1$ repeated eigenvalues of $-x$,
which when combined with the 0 eigenvalue shown, and the $n - (x + y)$ trivial $0$
eigenvalues coming from rows that are all zero, we get $n-1$ total eigenvalues,
meaning there is 1 more before we have a general solution.

This must correspond to the third eigenvector we get in the simple 3-tree case:

$
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
=
-3
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
$

If we guess that the general form is $<a, a\ldots a, b, b\ldots b>$ so that the
a repeats y times, and the b repeats x times, then upon application of M as
above, we get

\[ {[Mv]}_i = \begin{cases}
	-y*a + y*b & \text{if } i <= x\\
	x*a - x*b & \text{if } i > x
\end{cases} \]

Then if we suppose this vector $Mv$ equals $\lambda v$ then that gives two
equations:

\begin{equation}
	\lambda a = (yb - ay)
\end{equation}
\begin{equation*}
	\lambda b = (ax - xb)
\end{equation*}

Next we eliminate $\lambda$ and start to solve for $b$

\begin{equation*}
	(yb - ay)/a = (ax - xb)/b\\
\end{equation*}
\begin{equation*}
	b^2y - aby = a^2x - abx\\
\end{equation*}
\begin{equation*}
	b^2y + ab(x - y) - a^2x = 0
\end{equation*}
\begin{align*}
	r &= \frac{-a(x-y) \pm \sqrt{{a(x-y)}^2 + 4a^2xy}}{2y}\\
	  &= a\frac{y - x \pm (x + y)}{2y}\\
	  &= a or a\frac{-x}{y}
\end{align*}

$b = a$ corresponds to the zero-rowsum eigenvector from above, so it is not
new.
$b = a\frac{-x}{y}$ is new however, so set $a = y$, $b = -x$.

then our eigenvalue $\lambda$ can be derived from (1):
\begin{align*}
	\lambda &= \frac{y(b - a)}{a}
			&= -(x + y)
\end{align*}

This is our last eigenvalue, corresponding to the following eigenvector:

$\langle y, y\ldots y, -x, -x\ldots -x\rangle$

where $y$ is repeated $x$ times, and $-x$ is repeated $y$ times.

This means that all of our basis vectors are fully diagonalizable, as we have
$x+y$ nontrivial eigenvectors, plus $n-x+y$ empty rows, giving a total of $n$
eigenvectors.

\subsection{Simultaneous Diagonalization}

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices at once.

If we look at each matrix $L_i$, we can observe that the last eigenvalue in our
list above, $\lambda = -(x + y)$ is unique, and hence the eigenvector that
corresponds to it is uniquely determined by it. (modulo scaling)

Further in the matrix corresponding to the root of the tree, the $0$ eigenvalue
is also unique, so this node will actually give 2 uniquely determined
eigenvectors.

So in total we have determined $n$ eigenvectors!

Inspection of these shows that none of them are proportional to eachother,
since they all contain zeroes or negatives in different places to eachother.

Then we know that:
\begin{itemize}
	\item a set of linearly independent eigenvectors must exist,
	\item each of our eigenvalues is proportional to one of these vectors
	\item none of our eigenvalues are proportional to eachother
\end{itemize}
From this we can conclude that this must be a rescaling of some linearly
independent set of simultaneous eigenvectors, and hence is also a linearly
independent set of simultaneous eigenvectors.

If we label each eigenvector with the basis matrix that determined it, plus
$v_0$ as the zero-rowsum eigenvector, then the the eigenvalues of these
eigenvectors has a novel relationship with the structure of the binary tree:
\begin{itemize}
	\item $L_iv_i = -(x_i + y_i)v_i$ noting $v_i$ and $L_i$ come from the same
		node
	\item $L_iv_j = -x_iv_j$ if $j$ sits on the left subtree under $i$
	\item $L_iv_j = -y_iv_j$ if $j$ sits on the right subtree under $i$
	\item $L_iv_j = 0$ if $j$ sits outside of the subtree under $i$, or $j=0$
\end{itemize}

These results follow from constructing $v_j$ as a linear combination of the
eigenvectors described previously for $L_i$

So we have the exact value of our $n$ eigenvectors and eigenvalues.

\subsection{Ordering the Nodes}

If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular. (Once the zero-rowsum
column is removed)

One consequence of this is that the determinant will simply be

\[\prod_{i=1}^n d_i\]

This is simply the pre-order of the nodes, and so we have a closed form for the
determinant.

As an example of what all of the eigenvectors and eigenvalues look like in
matrix form, take two trees with 4 leaves:

\Tree[. [. 1 2 ] [. 3 4 ]]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 2 & 1 & 0\\
	1 & 2 & -1 & 0\\
	1 & -2 & 0 & 1\\
	1 & -2 & 0 & -1
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -2 & -2\\
	0 & 0 & -2 & 0\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]



\Tree[. [. [. 1 2 ] 3 ] 4 ]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 1 & 1 & 1\\
	1 & 1 & 1 & -1\\
	1 & 1 & -2 & 0\\
	1 & -3 & 0 & 0
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -1 & -1\\
	0 & 0 & -3 & -1\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]

\section{Deciding Fairness}

\subsection{Ubiquity in Eigenvalue Matrix}

The reference paper ``ubiquity of synonymity'' used an induction-like argument,
that if two trees are synonymous, then they sometimes also have an `exchange'
property, that adding branches around those trees will create further pairs of
synonymous trees.
%not really induction like now that i think about it

Then as tree size gets larger, the probability of containing a subtree with the
exchange property goes to 1, and hence so does the probability of another tree
existing with the same measurement.

In that case the measurement being used was the spectrum of a direct matrix
representation of the tree, but in our case one possible measurement is the
spectrum of the matrix of eigenvalues we have constructed.

This measurement for us immediately has the exchange property, as the spectrum
of our upper triangular matrices is simply the diagonal, and this diagonal is
simply the number of leaves in each subtree.

So then as long as there is a pair of trees with the same spectrum, we'll get
the same strong negative result for the spectrum.

\Tree[.    [. [. 1 2 ] [. 3 4 ]] [. 5   [. [. 6 [. 7 [. 8 9 ]]]]]]
\Tree[. [. [. [. 1 2 ] [. 3 4 ]]    5 ] [. [. 6 [. 7 [. 8 9 ]]]]]

As a result any measurement which is a symmetric function of the diagonal, such
as the determinant or the trace, will be at least as likely to coincide.

%note about these functions being (known?) measurements of balance

The two exceptions to this are, the trace of the eigenvalue matrix when not
triangular, and the spectrum of the eigenvalue matrix with ordering
information.

\subsection{Fairness of Eigenvalue Matrix}

It looks like both the eigenvector matrix, and the eigenvalue matrix, can be
used independent of eachother to reconstruct the tree structure.

The eigenvector matrix can be used to recover the leaf labels as well, whereas
the eigenvalue matrix can be used to recover the branch labels.

We know that the eigenvalue matrix is a fair representation of the tree with
internal nodes labelled, so long as we can reconstruct the tree and labels from
the eigenvalue matrix.

The eigenvalue matrix has 3 non-redundant sets of data in it:
\begin{itemize}
	\item The most extreme number in a row indicates the number of leaves under
		a node
	\item The column of that most extreme number indicates the eigenvector
		labelling
	\item Nonzero cells indicate that that column's node exists in the subtree
		of that row's node
\end{itemize}

So the first thing one could do is decompose the matrix into those 3 functions.

Define $W$ to be the matrix of eigenvalues, \emph{without the zero-rowsum
column}, and with negated (and hence positive) entries.

Define $I$ to be the set of integers $1$ thru $n-1$, i.e. $\mathds{Z} \cap [1,
n-1]$

\[\mathit{Size}(i) := \max\{W_{ij} | j \in I\}\]
\[\mathit{Col}: I \rightarrow I\]
\[\forall i \in I, W_{i \mathit{Col}(i)} = \mathit{Size}(i)\]
\[j \preccurlyeq i \Leftrightarrow W_{i \mathit{Col}(j)} > 0\]

$\mathit{Col}$ is well defined since we know $\mathit{Size}(i)$ is the unique
eigenvalue of $L_i$.

$\preccurlyeq$ is the original tree understood as a partial order, but with
leaves removed.

Simply reconstructing the tree from this partial order, and supplementing nodes
with leaves until it is a binary tree, should give the original tree.

On the other hand, one can use the $\mathit{Size}$ function, by taking the two
largest descendants as the children of each node, again supplemented with
leaves until each node has 2 children.

We know that this is in fact the original tree, as we've shown that the
eigenvalues of each matrix correspond to the structure of the tree.
% TODO show that the eigenvalues of each matrix correspond to the structure of
% the tree a bit more rigorously

\subsection{Fairness of Eigenvector Matrix}

The construction here is simple, if we use the eigenvector column indeces as
labels for the internal nodes, then the partial order is defined as:
\[j_1 \preccurlyeq j_2 \Leftrightarrow \forall i \in J, V_{ij_1} > 0
\Rightarrow V_{ij_2} > 0 \]

Then as before each node neads to be supplemented with any leaves, taking their
labels from the row indeces of $V$.

\section{loose notes}

\subsection{proof notes}

theres a bunch of things that could help some of the proofs above become more
clear.

First eigenvectors with non-zero eigenvalues have $v_{j} \neq 0 \Rightarrow$
leaf $j$ is a descendant of node $i$

In particular for the most extreme/unique eigenvalue the converse is also true.

Recovering tree structure from the vectors is then simple.
Recover the set of leaf descendants from above, then all if the descendants of
one node are also descendants of another node, the former is a descendant of
the latter.

Further we know that none of these can be proportional to eachother, or they
would belong to the same node, and hence be the same anyway.
The exception here is the zero-rowsum eigenvector, but we know none of the
constructed eigenvectors are proportional to that because its eigenvalue is
always 0.

Second the non-unique eigenspaces of the matrices correspond to tree structure
as well.
Leaves that dont descend from the node form an eigenspace on 0.
leaves that descend from the left child, restricted to have a sum of zero, form
An eigenspace on the size of the left subtree.
Likewise for the right child.

Then each of the eigenvectors in our set will sit in one of these three spaces
for all other nodes in the tree, which is very convenient.

This gives us the structural information we need to show that the eigenvalue
matrix is a fair representation of the original tree.

The fact that a binary tree can be reconstructed from the tree of its internal
nodes is probably a reference?

\subsection{minimum determinant}

each row of a balanced tree with $2^n$ leaves, is $2^i$ lots of $2^{n-i}$\ldots right?

\subsection{Meeting 2}

the determinants seem to measure how balanced the trees are, from maximally
balanced: (expr?), to maximally unbalanced: n!

The original question still remains, how much could be removed from the matrix
without breaking injectivity. Could you reconstruct the tree from an ordered
spectrum?

\subsection{eig matrices}

why do the two matrices need leaf/branch info to recover the original? could
you get rid of it? if you cant is it impossible to use these matrices to
distinguish tree shapes?

well chosen matrix norms of either matrix might stifle the exchange property
matrix differences might be useful once the matrices have been sorted? but then
a tree will not have 0 distance from its own relabellings.

\subsection{eigenvalue polynomials}

the eigenvectors can be discovered through a polynomial process, by looking at
the product of each pair of basis matrices, as a linear combination of the
basis

\subsection{hierarchies}

the eigenvector subset argument could be quite simple if described in terms of
hierarchies?

\subsection{Symmetries under relabelling}

The eig spaces of the basis matrices all have a sum of zero, except the all-1
eigenvector

corresponds exactly to the symmetries of the tree under the action of leaf
relabellings.

In particular each node is agnostic to relabellings of its left subtree, and of
its right subtree, so each node restricts the eigenspaces only with local
knowledge, until eventually the full structure of the tree has been
represented.

\subsection{Different kind of symmetric function}

When trying to summarize the eigenvalue matrix, we specifically want an
operation that is symmetric under row permutations, and under column
permutations, but not on arbitrary value permutations.

$f(X) = f(K_1XK_2)$

Then we can take further restrictions like ``must be homogeneous'' or whatever.

Under the wrong restrictions, we should get imanant polynomials out, but
steering clear of that might give something useful?

\end{document}
