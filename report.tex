\documentclass{report}
\usepackage{amsmath, dsfont, qtree}
\begin{document}

\section{Abstract}

We investigate representations of binary trees related to a particular matrix
algebra representation demonstrated previously.

[finish report first then finish abstract]

\section{Constructing Measurements of the Algebra}

\subsection{Background}

The paper ``Ubiquity of synonymity'' dealth with 3 different matrix
representations of trees: the adjacency matrix, laplacian matrix, and distance
matrix.
These representations are interesting, since relabellings of the tree result
in similar matrices, so their spectra will not change.
This means the spectra are themselves a representation of the tree, but the
paper shows that these representations are not fair, and in fact `approach'
triviality as the number of leaves increases.

A motivation indicated by the paper is to derive a distance function between
the structure of two trees.
Such a function would still have potential use if it was derived from an unfair
representation, but the result that representations become near trivial means
most trees would also have zero distance
As such while it would be nice to have a fair representation, it is crucial
that it is at least substantially non-trivial.

The Tas Phylo group has shown in another paper that trees can be represented as
an algebra of matrices, and that this algebra is commutative.
% algebra definition?
% proper references?

Similar to the previous paper, these matrices relate rows/columns to leaf
labels, and hence are similar to matrices associated with relabellings of the
tree.
Further, since the algebra is commutative, we have the property that if it
diagonalizable at all, then it is simultaneously diagonalizable.
Together these properties suggest that the spectra of matrices in this algebra
might be useful.

\subsection{Canonical Forms of Spectra}

If we assume that the matrices in the algebra are diagonalizable for now, (we
can show this later) then we can start to reason about the spectra of these
matrices.
With this assumption, it will also follow that all of the matrices in the
algebra are simultaneously diagonalizable, and hence that a set of eigenvectors
can be found common to the whole algebra.
We shall therefore refer to these as the Algebra's eigenvectors.

With this in place, we consider the spectra of the matrices in the algebra.
When representing each tree as a single matrix, comparing the spectra of these
matrices was a simple matter of comparing the multiplicity of each eigenvalue.
This could be thought of as either comparing the spectrum as a multiset, or as
a sorted sequence of values.
Then since the order of the eigenvectors doesn't matter, we can freely relabel
trees without changing their representation.
%     I keep changing my wording, I should try to stick to the original wording
% of the reference papers. "invariance under relabelling" etc.

We, on the other hand, are considering a multidimensional space of matrices,
and it might be nice to have this same property of invariance under
relabelling.
As such we might consider the set of sorted spectra of the whole algebra.

This representation seems to be useful, but once we consider the simultaneous
diagonalization of the algebra, we see a missed opportunity to generate a
homomorphism from the algebra to the vector space $\mathds{C}^n$.
That is, by taking the algebra's eigenvectors under some ordering, and
generating the spectra of matrices corresponding to these eigenvectors, we will
get the property that the spectrum of a linear combination of matrices is the
same linear combination of the individual spectra.
This means the ``spectrum'' map is a homomorphism, and so its image will be a
vector space.

This will not apply to the sorted image, e.g. $<1, 1> - <0, 1> = <1, 0>$
	% <tuple> notation?
So as an alternative, we could consider this vector space, and look for
properties that are invariant under relabelling of the original tree, and
invariant under different choices of eigenvector.
This is the same as finding properties that are invariant under permutation of
the axes of the vector space.

% note that it might look like we have strayed from any sense of distance formula
% measurements later on will have potential for distance formulae

\subsection{Subspace Triviality}

When looking at this vector space, the most obvious measurement we could apply
to it is its dimension.
It will turn out that its dimension is always the same as the algebra, which
seems odd when the algebra seems to contain matrices that are just relabellings
of eachother.
As an example every cherry in a tree will correspond to a matrix equivalent to
the following:

\[
	\left[ \begin{matrix}
		-1 & 1\\
		1 & -1
	\end{matrix} \right]
\]

The exact matrices are simply the above with added rows and columns of zero.
The spectrum of any matrix in this form should be the same as any other, and in
the sorted-list representation they will be, but our vector space will
distinguish these, in the same way that $<0, 1>$ and $<1, 0>$ are linearly
independent.

This means that symmetries in the tree might have created interesting
redundancies in the set of sorted spectra, but our vector space will not
reflect these symmetries in the same way.

In particular if we say that the number of leaves on the original tree is $n$,
then our algebra will be generated from $n-1$ internal nodes, and will be a set
of $n \times n$ matrices.

This means that the vector space will be an $n-1$ dimensional subspace of
$\mathds{C}^n$.

Unfortunately since the matrices all have zero row-sum, that corresponds to an
eigenvector of all 1s, and an eigenvalue of zero, which means the subspace will
simply be $\mathds{C}^{n-1}$ extended by a zero, regardless of the tree in
question.

Not only is the vector space not a fair representation of the binary tree, it
is a trivial representation.

\subsection{Parameter-Free Representations}

While it is not useful to map the whole algebra to $\mathds{C}^n$, if we just
look at the basis it turns out to be more useful.
% addressing the fact that other bases exist than the one the algebra was
% defined with? once we're talking about determinants this can be dealt with
% specifically, but the trace and spectrum of the eigenvalue matrix are heavily
% dependent on the basis chosen
As such we shall consider the basis that was used to define the algebra.

If the original tree has $n$ leaves, then we have
\begin{itemize}
	\item $n-1$ internal nodes
	\item $n-1$ basis matrices
	\item $n$ common eigenvectors
	\item $(n-1) \times n$ eigenvalues
\end{itemize}

This could be summarized in a pair of matrices:
\begin{itemize}
	\item An $n \times n$ matrix of eigenvectors, and
	\item an $(n-1) \times n$ matrix of eigenvalues.
\end{itemize}

This second matrix will always have a column of zeroes in it, corresponding to
the eigenvector of all 1s, since the algebra has zero row-sum.
As such we do not lose any information by removing this column, and getting a
square $(n-1) \times (n-1)$ matrix.
We could do the same for the eigenvector and get an $n \times (n-1)$ matrix,
but obviously this has the opposite effect.

These matrices are not invariant under relabelling:
\begin{itemize}
	\item relabelling leaves permutes the rows of the eigenvector matrix
	\item relabelling internal nodes permutes the rows of the eigenvalue matrix
	\item relabelling eigenvectors permutes the columns of both matrices.
\end{itemize}

It seems like either of these matrices on their own would be enough to
reconstruct the algebra, and hence the tree, so on their own they are
interesting representations of a tree.
Further still, measurements that are invariant under the row/column
permutations above have potential for distinguishing tree-shapes as was
originally desired.

One clear possibility is to take the determinant of either of these matrices,
and remove the effect of row/column permutations with an absolute value.
For now we shall derive the exact value of both of the matrices.

% not included currently, could be a useful analogy:
% Geometrically we can consider this to be the volume spanned by the spectrum vectors.

\section{Simultaneous Diagonalization}

%\subsection{basis variance}
%
%If $detsubst(L*) =$ the determinant as described, but with respect to basis
%$L*, L_2, L_3\ldots L_n-1$, so that $detsubst(L_1)$ is exactly the
%determinant as described.
%
%Then $detsubst(L_1 + L_2)$ expanded on the first column seems to give the same
%expression as $detsubst(L_1) + detsubst(L_2)$ expanded on their first columns.
%
%But $detsubst(L_2)$ is the determinant of a singular matrix:
%\[[spec(L_2), spec(L_2), spec(L_3)\ldots]\]
%so equals zero.
%
%The same doesn't seem to apply to $detsubst(L_1 + L_1)$ though, so we might
%need to normalize the $L_i$ matrices somehow?


\subsection{Eigenvalues}

When working towards a simultaneous diagonalization of the algebra, the first
thing we need is to show that the matrices can be diagonalized at all.
In showing this we expect to get the eigenvalues as well.

For simplicity we will assume that the tree leaves have been labelled ``from
left to right'', i.e.\ such that at any internal node, its left descendants are
less than its right descendants.

Formally, that there is a total order of the tree such that the ``left'' child
of any node (being the minimum of the two children) has descendents that are
all less than the ``right'' child of that node.

Other labellings will simply permute the coordinates of the eigenvectors we
derive here.

Then we can see that the matrix that corresponds to a node with $x$ left
descendents, $y$ right descendents, and minimum descendent $l+1$. (so that
there are $l$ leaves to the left of the node's minimum descendent)

\[ M_{ij} = \begin{cases}
-y & l < i = j \leq l + x\\
-x & l + x < i = j \leq l + x + y\\
1 & l < i \leq l + x < j \leq l + x + y\\
1 & l < j \leq l + x < i \leq l + x + y\\
0 & otherwise
\end{cases} \]

So for example if we take the tree:

\Tree[. 1 [. [.i [. 2 3 ] 4 ] 5 ]]

Then the node marked $i$ will have
$l=1$, $x=2$, $y=1$
which gives
\[ L_i = M = \left[ \begin{matrix}
	0 & 0 & 0 & 0 & 0\\
	0 & -1 & 0 & 1 & 0\\
	0 & 0 & -1 & 1 & 0\\
	0 & 1 & 1 & -2 & 0\\
	0 & 0 & 0 & 0 & 0
\end{matrix} \right] \]

From this it becomes clear that all but $x+y$ of the eigenvalues will be zero,
and zero-rowsum brings this down to $x+y-1$ non-zero eigenvalues.

As a side note this means that the matrix of all of the eigenvalues will be
at least half zeroes, which simplifies a lot of calculations on this
matrix, especially once we find a way of permuting the eigenvalue matrix to be
upper triangular.

\subsection{closed form eigenvectors}

given the simple $M_{ij}$ form above, it becomes possible to find eigenvectors
for these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
=
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
\left[\begin{matrix}
	0 & 0 & 0\\
	0 & -1 & 0\\
	0 & 0 & -3
\end{matrix}\right]
\end{equation*}

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
=
-1
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
\end{equation*}

In the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
In the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of $<1, -1, 0\ldots 0>$ will generalize to any $M_{ij}$ as above, as long as
$x \geq 2$, with an eigenvalue of -y
similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of $<0\ldots 0, -1, 1>$ with an eigenvalue of -x
further still we can get $x-1$ of the former and $y-1$ of the latter by moving the
-1 coordinate to any other row $\leq$ x, and the above argument would still apply.

This gives $x-1$ repeated eigenvalues of $-y$, $y-1$ repeated eigenvalues of $-x$,
which when combined with the 0 eigenvalue shown, and the $n - (x + y)$ trivial $0$
eigenvalues coming from rows that are all zero, we get $n-1$ total eigenvalues,
meaning there is 1 more.

This must be the third eigenvector we get in the simple 3-tree case:

$
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
=
-3
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
$

it's hard to understand exactly why this is an eigenvector, but if we guess
that the general form is $<a, a\ldots a, b, b\ldots b>$ so that the a repeats y
times, and the b repeats x times, then upon application of M as above, we get

if $i <= x$
then ${(Mv)}_{ij} = -y*a + y*b$
if $i > x$
then ${(Mv)}_{ij} = x*a - x*b$

then if we suppose that these equal l*a and l*b respectively, we get:

\begin{equation*}
	(yb - ay)/a = (ax - xb)/b\\
\end{equation*}
\begin{equation*}
	b^2y - aby = a^2x - abx\\
\end{equation*}
\begin{equation*}
	b^2y + ab(x - y) - a^2x = 0
\end{equation*}
let $r = \frac{b}{a}$
\begin{align*}
	r &= \frac{-(x-y) \pm \sqrt{{(x-y)}^2 + 4xy}}{2y}\\
	  &= \frac{y - x \pm (x + y)}{2y}\\
	  &= 1 or \frac{-x}{y}
\end{align*}

$r = 1$ corresponds to our common eigenvector from above, so it is not new.
$r = \frac{-x}{y}$ is new however, so set $b = -x$, $a = y$,

then our eigenvalue l:
\begin{align*}
	l &= \frac{y(b - a)}{a}
	  &= -(x + y)
\end{align*}

This is our last eigenvalue, corresponding to the following eigenvector:

$\langle y, y\ldots y, -x, -x\ldots -x\rangle$

where y is repeated x times, and x is repeated y times.


\subsection{Simultaneous Diagonalization}

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices.

If we look at each matrix $L_i$, we can observe that the last eigenvalue in our
list above, $\lambda = -(x + y)$ is unique, and hence the eigenvector that corresponds
to it is uniquely determined by it. (modulo scaling)

Since each matrix will give a different, uniquely determined vector for its
most unique eigenvalue, we have $n-1$ eigenvectors, which are also distinct from
our other uniquely determined vector $<1, 1\ldots 1>$.

So we know that that all of our eigenvectors are determined, and can use this
set of eigenvectors for diagonalization of any basis matrix.


If we number these vectors $v_i$, and set $v_0$ to be $<1, 1\ldots 1>$, then we can
understand which eigenvalues will correspond to different eigenvectors.

For $L_i$, $v_i$ will by definition give $L_i v_i = -(x+y) v_i$

for nodes descending from the left child of i, $L_i v_l = -y v_l$
and similarly on the right, $L_i v_r = -x v_r$
this can be seen both by the quantities of nodes, and more rigorously by the
fact that each of these $v_l$ is a linear combination of vectors constructed
above, with eigenvalue -y, and similarly for $v_r$ and $-x$

for nodes above and anywhere else, the nontrivial part of $L_i v_j$ will be
entirely within one of the four consistent sections of $v_j$, so $v_j$ might as
well be a multiple of $v_0$, giving $L_i v_j = 0 v_j$

So we know all n of our eigenvalues, and all of their corresponding
eigenvectors.

\subsection{Simultaneous Diagonalization Proof}

Since we can diagonalize any matrix in the basis, we can rely on the existence
of a simultaneously diagonalizing set of eigenvectors. Call this $U$.
With this set we can prove that each of our concrete eigenvectors is
proportional to a vector in $U$.

Then for each matrix $L_i$, we know that its $i$th eigenvalue is unique, so set
$u_i$ to be the eigenvector in $U$ with this eigenvalue, so that $v_i$ is
proportional to it.
We know this $u_i$ must exist and be unique otherwise $U$ would not be linearly
independent.

Similarly take $u_0$ to be the element of $U$ proportional to $v_0$. We know
this exists and is unique because it corresponds to the unique eigenvalue of
$0$ in $L_1$. % or whatever the root node is... this is getting so wordy

Then we need to show that this set $u_0$ through to $u_n$ is exactly $U$.
This is true provided that no two $u_i = u_j$ unless $i = j$.

So suppose that $u_i = u_j$ for some $i, j \in \mathds{Z} \cap [0, n]$
Then since $v_i$ is proportional to $u_i$ and similarly for $j$, we know that
$v_i$ and $v_j$ must be proportional to \emph{eachother}.

Since all of the eigenvectors we have found have zero values for all leaves
outside of some subtree of the tree, we can simply use the tree-structure to
see that $v_i$ needs to be derived from the same subtree as $v_j$.

So unless one of the vectors is $v_0$ we know that $i = j$

Then if one is $v_0$ and the other is the eigenvector corresponding to the root
node, they clearly aren't proportional, as one contains positives and
negatives, while the other only contains zeroes.

%This proof is everything I hate in the world, I'd rather just cut it completely

\subsection{Returning to the determinant}


If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular, and so its determinant will
simply be

\[\prod_{i=1}^n d_i\]

this is simply the pre-order of the nodes, and so we have a closed form of the
determinant.

The reference paper ``ubiquity of synonymity'' used an induction-like argument,
that if two trees are synonymous, then adding branches around those trees will
create further pairs of synonymous trees, which looks almost trivial to show
for this spectral determinant.

As such if there is any synonymity then as the paper showed, it will become
ubiquitous for large enough n.

It seems unlikely that there would be \emph{no} synonymity, which is the only way
for there to be \emph{anything less than ubiquitous} synonymity.


\subsection{minimum determinant}

each row of a balanced tree with $2^n$ leaves, is $2^i$ lots of $2^(n-i)$\ldots right?

\section{Meeting 2}


looking at spectra generated for trees of size 4 and 5 confirmed the result
that spectral matrices are upper triangular, and that diagonals are the
negative of the size of the subtree corresponding to that node

the determinants seem to measure how balanced the trees are, from maximally
balanced: (expr?), to maximally unbalanced: n!


However since the matrices are triangular, it does look like the determinants
coincide at 8 leaves
another permutation-independant measure is the trace, which coincides at 6
leaves.

This discussion splits two ways, first is the fact that the trace does depend
on how eigenvectors are associated with basis matrices, changing the relation
creates different trace values.
Second is the fact that the trace and the determinant are symmetric functions
of the diagonal, i.e.\ of the eigenvalues \emph{of the spectral matrix}.

It turns out that there is a pair of trees with 9 leaves that have their whole
diagonal the same! (modulo ordering)
this means that any function which is symmetric (and hence doesn't depend on
relabelling) will be non-injective as well.

On the other hand the matrix as a whole appears to be injective, so it is worth
asking how much could be removed from the matrix without breaking injectivity.
Could you reconstruct the tree from an ordered spectrum?


\end{document}
