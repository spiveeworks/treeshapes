\documentclass{report}
\usepackage{amsmath, amssymb, amsthm, dsfont, qtree}

\DeclareMathOperator{\MRCA}{MRCA}

% used when defining tree shapes as orbits of a group action
\DeclareMathOperator{\relabel}{relabel}

\begin{document}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

%\section{Abstract}

%We investigate representations of binary trees related to a particular matrix
%algebra representation demonstrated previously.

%[finish report first then finish abstract]

\section{Ubiquity of Synonymity}

This project is based on the paper ``Ubiquity of synonymity: almost all large
binary trees are not uniquely identified by their spectra or their immanantal
polynomials''.

This paper shows that three different matrix representations of a binary tree
fail to distinguish different trees based on, as the title would suggest, their
spectra or their immanantal polynomials.

\subsection{Representations}

The 3 matrix representations are the Adjacency matrix, the Laplacian matrix,
and the Distance matrix.

The adjacency matrix takes the tree as an undirected graph, represented simply
as an adjacency matrix.

The laplacian matrix is the adjacency matrix but with the diagonal changed so
that the matrix has zero row and column sum.

The distance matrix is a smaller matrix where $D_{ij}$ is the length of the
shortest path from the $i$th leaf to the $j$th leaf.

All of these representations are one to one representations of the tree except
for the distance matrix.  

Relabelling vertices in the tree/graph will simultaneously permute
corresponding rows and columns of all of these representations.  

This means that the immanant and spectrum have a lot of potential as
representations, since they can potentially give the same result under
permutation, but different results if two trees really have different shapes.

Of course this does not turn out to be the case, which is the point of the
paper.

This is shown by making 3 important arguments, from which the failure of these
results can be directly inferred.

\subsection{Results}

The first result of the paper is that adjacency/laplacian matrices with the
same spectrum will have the same immanantal polynomials and vice versa.

The second result of the paper is the definition and existence of an exchange
property, wherein some trees not only have the same spectrum, but any tree that
contain those subtrees will also have the same spectrum. (this result applied
to both distance matrices, and to emph{linear combinations} of
adjacency/laplacian matrices)

The third result is that the proportion of trees that contains a specified
subtree will approach 1 as tree size gets larger.

\subsection{Conclusions}

Clearly from this the paper has shown that the following 5 functions are what
we will later define as ``near trivial'' representations:
\begin{itemize}
	\item immanantal polynomial of adjacency matrix
	\item immanantal polynomial of adjacency matrix
	\item spectrum of adjacency matrix
	\item spectrum of laplacian matrix
	\item spectrum of distance matrix
\end{itemize}

To say that a representation is ``near trivial'' only says that for large
enough trees the representation will have a lot of distinct trees with the same
value.

In particular the paper showed that the rate of approach is fairly slow, so as
long as your trees are small enough or your use case is sufficiently permissive
of collisions, or some combination of the two, these 5 measurements might still
be viable.

Otherwise these representations will not help.

\subsection{Usage in This Project}

The three main points we use from this paper are as follows:
\begin{enumerate}
	\item Trees have many convenient representations that are invariant under
		relabelling and many that are able to distinguish trees that have
		different shape, but not many with both
	\item Matrix spectra are a promising way of removing permutations from a
		matrix without reducing the matrix to a single number
	\item If a tree representation has an exchange property for some pair of
		trees, then the representation will be near trivial for large trees.
\end{enumerate}

\section{Tree Construction}

Trees will be represented as directed graphs, that is a set of ordered pairs.

\begin{definition} Predecessor/Child

	Given a graph $G$ over verteces $V$, and a vertex $v \in V$, the
	predecessors of $v$ are the set $P_v \subset V$ with $u \in P_v
	\Leftrightarrow (u, v) \in G$

	Similarly the children $C_v$ are given by $(v, u) \in G$
\end{definition}

The following definitions are not used other than to define a binary tree.
[this may change as later reconstruction of trees becomes more rigorous]

\begin{definition} Path

	A path is an ordered list of vertices with the property that adjacent
	vertices are also adjacent in the tree.

	\begin{itemize}
		\item $[v]$ is a trivial path in $G$ if $v \in V$
		\item $[u, v, \ldots]$ is a path in $G$ if and only if $(u, v) \in G$
			and $[v, \ldots]$ is a path in $G$
		\item if neither of the above cases are met, the list is not a path
	\end{itemize}
\end{definition}

\begin{definition} Cycle

	A cycle is simply a path with at least 2 elements, and equal endpoints
\end{definition}

\begin{definition} Binary Tree with $n$ leaves

	A graph is binary tree with $n$ leaves if and only if
	\begin{itemize}
		\item there are no cycles
		\item every node has exactly one predecessor, except one node called
			the root, which has no predecessor
		\item every node has either 0 children (a leaf) or 2 children
			(a branch)
		\item there are exactly $n$ leaves
	\end{itemize}
\end{definition}

For convenience we will also assume that the tree is labelled with leaves $1$
through to $n-1$, hence leaves are labelled $n$ through to $2n-1$.

Hence define $B$ to be the set of branch labels.
\[\{b | b \in \mathds{Z}, 1 \leq b < n\}\]
Similarly $L$ to be the leaf labels.
\[\{l | l \in \mathds{Z}, n \leq l < 2n-1\}\]

\subsection{Most Recent Common Ancestor}

\begin{definition} Ancestry

	If a node r is the root of a tree, then its ancestry is simply the
	singleton ordered list $(r)$.

	If a node r is not the root of a tree, then its ancestry is acquired by
	taking the ancestry of its predecessor, and prepending r itself.

	In other words the ancestry of a node is the list of nodes above that node,
	starting with itself, and ending with the root of the tree.
\end{definition}

\begin{definition} Common Ancestry

	The common ancestry of two nodes is the largest ordered list of nodes that
	is a suffix of both nodes' ancestries.
\end{definition}

\begin{definition} Most Recent Common Ancestor

	The MRCA of two nodes (in particular two leaves) is the first node in the
	common ancestry of the two nodes.
\end{definition}

% is it ok to take graph concepts like predecessor for granted
% also self explanatory concepts used later such as descendants, children,
% descendant leaves, etc.

\subsection{Relabelling}

Trees can be relabelled by injective functions on their node labels.

\begin{definition} Relabel

	We can use a permutation $\sigma \in S_{2n-1}$ to relabel a tree by taking
	the group action of $S_{2n-1}$ on a graph.

	\[
		\relabel(\sigma, G) := \{(\sigma(i), \sigma(j)) | (i, j) \in G\}
	\]
\end{definition}

Just as group actions give us a concept of relabelling, the orbits of these
actions will give a set of trees that are relabellings of eachother.

This can give us an exhaustive set of trees with the same structure, which we
can take to represent the structure itself:

\begin{definition} Tree Structure

	The structure of a tree with $n$ leaves, is simply the orbit of that tree
	under the $\relabel$ action of the group ${S_B}{S_L}$.

	Similarly the group $S_B$ could be thought of as that tree without branch
	labels, and $S_L$ as that tree without leaf labels.
\end{definition}

\subsection{Representation}

Our goal is to find measurements of trees that can identify if trees are
relabellings of eachother, and potentially how close they are to being
relabellings of eachother.

Such a measurement would, at least, map trees with the same structure to the
same values, and map trees with different structures to different values.

Hence we define the following concepts, noting some caveats:

\begin{definition} Invariance Under Relabelling

	A function or operation on binary trees with n leaves, is invariant under
	relabelling, if and only if trees with the same structure map to the same
	result.

	Similarly a function or operation is invariant under branch/leaf
	relabellings if trees that are equal with their branches/leaves removed,
	map to the same result.

	These properties could also be invoked ``up to isomorphism'' or up to some
	other class of operations.

	This simply means that the results of the function don't need to be exactly
	the same, they just need to be mapped to eachother by some operation in the
	specified class.
\end{definition}

\begin{definition} Distinction of Tree Shapes

	A function or operation on binary trees with n leaves, distinguishes tree
	shapes, if and only if trees with different structures map to different
	results.

	As before we also define distinguishing trees ignoring leaves/branches, and
	distinguishing all of these up to isomorphism/permutation/etc.
\end{definition}

Note that distinguishing tree shapes up to a class of operations is
\emph{stronger} than distinguishing tree shapes in general, whereas invariance
under tree shapes up to a class of operations is \emph{weaker} than
distinguishing tree shapes.

What we will find in this paper is that our representations either have both
properties up to row/column permutation of matrices, or fail to distinguish
tree shapes at all. (and become near-trivial as defined below)

In addition to the above properties, if a function doesn't distinguish tree
shapes, then there is a stronger failure condition shown in Matsen, which we
will call ``near triviality'':

\begin{definition} Near-Trivial

	A function or operation on binary trees with n leaves, is near trivial, if
	and only if the proportion of `failures to distinguish' approaches 1 as n
	gets larger.

	That is, if the proportion of pairs of trees that have different tree
	shapes but map to the same result, out of all pairs with different shapes,
	approaches 1.
\end{definition}

\begin{definition} Exchange

	Given a function that is invariant under relabelling, regardless of tree
	size, but does not always distinguish tree shapes, a pair of tree
	structures $A$ and $B$ `exchange' if they not only map to the same result
	under the function, but any tree that contains $A$ as a subtree will map to
	the same result as that tree with $A$ replaced with $B$.
\end{definition}

[Note that subtrees break our assumption that branch labels are 1..n-1, and
leaf labels are n..2n-1. It may be useful to explicitly define subtrees in
terms of \emph{tree structure} along with this concept of ``replacing'' a
subtree]

In Matsen it was shown that if a pair of trees exchange then the function for
which they exchange will be near-trivial.

\subsection{Hierarchy}

There is another way of representing a tree without its branch labels, which is
by representing it as a hierarchy based on sets of leaves under a node.

[briefly define hierarchies and maybe argue for their equivalence to trees
without branch labels]

\section{Constructing Measurements of the Algebra}

\subsection{Background}

The paper ``Ubiquity of synonymity'' dealth with 3 different matrix
representations of trees: the adjacency matrix, laplacian matrix, and distance
matrix.
These representations are interesting, since relabellings of the tree result
in similar matrices, so their spectra will not change.
This means the spectra are themselves a representation of the tree, but the
paper shows that these representations are not fair, and in fact `approach'
triviality as the number of leaves increases.

A motivation indicated by the paper is to derive a distance function between
the structure of two trees.
Such a function would still have potential use if it was derived from an unfair
representation, but the result that representations become near trivial means
most trees would also have zero distance
As such while it would be nice to have a fair representation, it is crucial
that it is at least substantially non-trivial.

The Tas Phylo group has shown in another paper that trees can be represented as
an algebra of matrices, and that this algebra is commutative.

The algebra is as follows:

\begin{definition} Algebra of a Tree

	Given a binary tree with $n$ leaves, that tree's algebra is an
	$n-1$-dimensional commutative algebra spanned by $n-1$ basis matrices
	corresponding to the $n-1$ branch nodes of the tree.

	The basis matrix $r$ in a tree with $n$ leaves has the following
	definition:
\end{definition}
	\[ L_{ij} = \begin{cases}
		1 & i \neq j \textit{and} \MRCA(i, j) = r\\
		0 & i \neq j \textit{and} \MRCA(i, j) \neq r\\
		-\sum_{k \neq j} L_{ik} & \textit{otherwise}
	\end{cases} \]

% proper references?

Similar to the previous paper, these matrices relate rows/columns to leaf
labels, and hence are similar to matrices associated with relabellings of the
tree.

That is, this algebra representation is invariant under relabelling up to
simultaneous row/column permutation.

It also distinguishes tree shapes up to row/column permutation.

As a set of matrices, the algebra no longer has branch information, however the
basis as an ordered set will still have branch information, so we can see that
the basis is not invariant under tree shapes, but will distinguish tree shapes,
and that the algebra will in fact be invariant under branch relabelling.

[also the basis is invariant under branch relabelling up to basis permutations,
invariant under leaf relabelling up to row/column permutation, these properties
seem too expressive to thoroughly explore]

Further, since the algebra is commutative, we have the property that if it
diagonalizable at all, then it is simultaneously diagonalizable.
Together these properties suggest that the spectra of matrices in this algebra
might be useful.

\subsection{Canonical Forms of Spectra}

If we assume that the matrices in the algebra are diagonalizable for now, (we
can show this later) then we can start to reason about the spectra of these
matrices.
With this assumption, it will also follow that all of the matrices in the
algebra are simultaneously diagonalizable, and hence that a set of eigenvectors
can be found common to the whole algebra.
We shall therefore refer to these as the Algebra's eigenvectors.

With this in place, we consider the spectra of the matrices in the algebra.
When representing each tree as a single matrix, comparing the spectra of these
matrices was a simple matter of comparing the multiplicity of each eigenvalue.
This could be thought of as either comparing the spectrum as a multiset, or as
a sorted sequence of values.
Then since the order of the eigenvectors doesn't matter, we can freely relabel
trees without changing their representation.
%     I keep changing my wording, I should try to stick to the original wording
% of the reference papers. "invariance under relabelling" etc.

We, on the other hand, are considering a multidimensional space of matrices,
and it might be nice to have this same property of invariance under
relabelling.
As such we might consider the set of sorted spectra of the whole algebra.

This representation seems to be useful, but once we consider the simultaneous
diagonalization of the algebra, we see a missed opportunity to generate a
homomorphism from the algebra to the vector space $\mathds{C}^n$.
That is, by taking the algebra's eigenvectors under some ordering, and
generating the spectra of matrices corresponding to these eigenvectors, we will
get the property that the spectrum of a linear combination of matrices is the
same linear combination of the individual spectra.
This means the ``spectrum'' map is a homomorphism, and so its image will be a
vector space.

\begin{definition}Spectral Space

	The spectral space of a binary tree will here mean the set of spectra of
	matrices from the tree's algebra.

	This set is a vector space since the algebra is commutative.
\end{definition}


This will not apply to the sorted image, e.g. $<1, 1> - <0, 1> = <1, 0>$
	% <tuple> notation?

This spectral space will be invariant under relabelling, up to permutation of
the axes of the space.

Additionally, since it was constructed from the algebra and not from an ordered
basis, it will be invariant under branch relabelling.

This seems like a promising representation; if we could find a convenient
measurement of the space to get rid of the axis labels, that would be very
useful.

One such measurement would be the dimension, but when we consider the dimension
we find something else about these spaces\ldots

% note that it might look like we have strayed from any sense of distance formula
% measurements later on will have potential for distance formulae

\subsection{Subspace Triviality}

%When looking at this vector space, the most obvious measurement we could apply
%to it is its dimension.
It will turn out that its dimension is always the same as the algebra, which
seems odd when the algebra seems to contain matrices that are just relabellings
of eachother.
As an example every cherry in a tree will correspond to a matrix equivalent to
the following:

\[
	\left[ \begin{matrix}
		-1 & 1\\
		1 & -1
	\end{matrix} \right]
\]

The exact matrices are simply the above with added rows and columns of zero.
The spectrum of any matrix in this form should be the same as any other, and in
the sorted-list representation they will be, but our vector space will
distinguish these, in the same way that $<0, 1>$ and $<1, 0>$ are linearly
independent.

This means that symmetries in the tree might have created interesting
redundancies in the set of sorted spectra, but our vector space will not
reflect these symmetries in the same way.

In particular if we say that the number of leaves on the original tree is $n$,
then our algebra will be generated from $n-1$ internal nodes, and will be a set
of $n \times n$ matrices.

This means that the vector space will be an $n-1$ dimensional subspace of
$\mathds{C}^n$.

Unfortunately since the matrices all have zero row-sum, that corresponds to an
eigenvector of all 1s, and an eigenvalue of zero, which means the subspace will
simply be $\mathds{C}^{n-1}$ extended by a zero, regardless of the tree in
question.

Not only is the vector space not a fair representation of the binary tree, it
is a trivial representation.

\subsection{Parameter-Free Representations}

While it is not useful to map the whole algebra to $\mathds{C}^n$, if we just
look at the basis it turns out to be more useful.
% addressing the fact that other bases exist than the one the algebra was
% defined with? once we're talking about determinants this can be dealt with
% specifically, but the trace and spectrum of the eigenvalue matrix are heavily
% dependent on the basis chosen
As such we shall consider the basis that was used to define the algebra.

If the original tree has $n$ leaves, then we have
\begin{itemize}
	\item $n-1$ internal nodes
	\item $n-1$ basis matrices
	\item $n$ common eigenvectors
	\item $(n-1) \times n$ eigenvalues
\end{itemize}

This could be summarized in a pair of matrices:
\begin{itemize}
	\item An $n \times n$ matrix of eigenvectors, and
	\item an $(n-1) \times n$ matrix of eigenvalues.
\end{itemize}

This second matrix will always have a column of zeroes in it, corresponding to
the eigenvector of all 1s, since the algebra has zero row-sum.
As such we do not lose any information by removing this column, and getting a
square $(n-1) \times (n-1)$ matrix.
We could do the same for the eigenvector and get an $n \times (n-1)$ matrix,
but obviously this has the opposite effect.

These matrices are not invariant under relabelling:
\begin{itemize}
	\item relabelling leaves permutes the rows of the eigenvector matrix
	\item relabelling internal nodes permutes the rows of the eigenvalue matrix
	\item relabelling eigenvectors permutes the columns of both matrices.
\end{itemize}

It seems like either of these matrices on their own would be enough to
reconstruct the algebra, and hence the tree, so on their own they are
interesting representations of a tree.
Further still, measurements that are invariant under the row/column
permutations above have potential for distinguishing tree-shapes as was
originally desired.

One clear possibility is to take the determinant of either of these matrices,
and remove the effect of row/column permutations with an absolute value.
As such we shall first derive the exact value of both of the matrices.

% not included currently, could be a useful analogy:
% Geometrically we can consider this to be the volume spanned by the spectrum
% vectors.

\section{Simultaneous Diagonalization}

%\subsection{basis variance}
%
%If $detsubst(L*) =$ the determinant as described, but with respect to basis
%$L*, L_2, L_3\ldots L_n-1$, so that $detsubst(L_1)$ is exactly the
%determinant as described.
%
%Then $detsubst(L_1 + L_2)$ expanded on the first column seems to give the same
%expression as $detsubst(L_1) + detsubst(L_2)$ expanded on their first columns.
%
%But $detsubst(L_2)$ is the determinant of a singular matrix:
%\[[spec(L_2), spec(L_2), spec(L_3)\ldots]\]
%so equals zero.
%
%The same doesn't seem to apply to $detsubst(L_1 + L_1)$ though, so we might
%need to normalize the $L_i$ matrices somehow?

When working towards a simultaneous diagonalization of the algebra, the first
thing we need is to show that the matrices can be diagonalized at all.
In showing this we expect to get the eigenvalues as well.


\subsection{Simple Matrix Format}

For simplicity we will assume that the tree leaves have been labelled ``from
left to right''.

As an example of a tree that isn't labelled left to right, take the following tree:

\Tree[.1 [.2 3 5 ] 4]

The issue we have is that $2$ has a descendant that belongs to the left of $4$ and another to the right of $4$, but $4$ is not itself a descendant.

\begin{definition} Left to Right

	A binary tree with $n$ leaves is labelled Left to Right if and only if the descendents of any vertex are a set of consecutive integers.
\end{definition}

Other labellings will simply permute the coordinates of the eigenvectors we
derive here.

Then if we consider a vertex in such a tree, and suppose that:
\begin{itemize}
	\item its descendents are in the range $(l, l + x + y]$,
	\item its left child's descendents are in the range $(l, l + x]$,
	\item its right child's are in the range $(l + x, l + x + y]$
\end{itemize}
where the above ranges are taken to be $subset \mathds{Z}$, then we can infer the following basis matrix corresponding to this vertex:

\begin{definition} The $M$ form of a basis matrix

	\[ M_{ij} = \begin{cases}
		-y & l < i = j \leq l + x\\
		-x & l + x < i = j \leq l + x + y\\
		1 & l < i \leq l + x < j \leq l + x + y\\
		1 & l < j \leq l + x < i \leq l + x + y\\
		0 & otherwise
	\end{cases} \]
\end{definition}

\begin{lemma} The $\MRCA$ of two leaves $i$ and $j$ is $r$ if and only if they are descendants of different children of $r$.
\end{lemma}

Proof:

If $i$ and $j$ descend from the same child of $r$, then that child will also be in the common ancestry of $i$ and $j$, and hence $r$ will not be the most recent one.

If $i$ and $j$ descend from different children of $r$, say $v_L$ and $v_R$ respectively, and we suppose that $j$ is \emph{also} a descendant of $v_L$, then the ancestry of $j$ would show either a path from $v_L$ to $v_R$ or the other way around.
A path ending in either $v_L$ or $v_R$ must contain $r$ penultimately, since vertices in a tree only have one predecessor, which means we can construct a non-trivial path from $r$ to itself.
Since such a cycle is also impossible, it must \emph{not} be the case that $j$ descends from $v_L$, and similarly we would see that $i$ does not descend from $v_R$.
In other words, neither one is a common ancestor of $i$ and $j$, and yet $r$ \emph{is} a common ancestor, so $r$ is the most recent common ancestor.

\begin{lemma} The $M$ form matrix on a node $r$ is a basis matrix
\end{lemma}

	%\[ L_{ij} = \begin{cases}
		%1 & i \neq j \textit{and} \MRCA(i, j) = r\\
		%0 & i \neq j \textit{and} \MRCA(i, j) \neq r\\
		%-\sum_{k \neq j} L_{ik} & \textit{otherwise}
	%\end{cases} \]
Proof:

Our basis matrix was defined by 3 cases, the first two of which:

\begin{equation} \label{L_eqn_yes}
	L_{ij} = 1 \textit{if} i \neq j \textit{and} \MRCA(i, j) = r
\end{equation}
\begin{equation} \label{L_eqn_no}
	L_{ij} = 0 \textit{if} i \neq j \textit{and} \MRCA(i, j) \neq r
\end{equation}

correspond to 3 of the cases in our $M$ form matrix:

\begin{equation} \label{M_eqn_lr}
	M_{ij} = 1 \textit{if} l < i \leq l + x < j \leq l + x + y
\end{equation}
\begin{equation} \label{M_eqn_rl}
	M_{ij} = 1 \textit{if} l < j \leq l + x < i \leq l + x + y
\end{equation}
\begin{equation} \label{M_eqn_no}
	M_{ij} = 0 \textit{otherwise, as long as} i \neq j
\end{equation}

We see that the conditions for \ref{M_eqn_lr} in a left to right labelled tree directly correspond to $i$ descending from the left child of $r$ and $j$ from the right child of $r$.
\ref{M_eqn_rl} instead shows these for $j$ and $i$ respectively.
\ref{M_eqn_no} takes up the remaining cases, so we conclude that in $M$ if $i \neq j$, then $M_{ij} = 1$ if and only if $i$ and $j$ descend from different children of $r$, and $M_{ij} = 0$ otherwise.

By the lemma above this means that $\MRCA(i, j) = r$ when $M_{ij} = 1$, and $MRCA(i, j) \neq r$ when $M_{ij} = 0$, so in these cases $M_{ij} = L_{ij}$

The remaining case is $i = j$, which in our basis matrix is:

\begin{equation} \label{L_eqn_diag}
	L_{ii} = -\sum_{k \neq i} L_{ik}
\end{equation}

By inspecting the cases of $M_{ik}$ one can see that if $i \leq l$ or $l + x + y < i$ then $L_{ii} = 0 = M_{ii}$
similarly if $l < i \leq l + x$ then $L_{ii} = -sum_{l+x < k \leq l+x+y} 1 = -y = M_{ii}$ and so on.

So $M$ is in fact the basis matrix corresponding to $r$.

So for example if we take the tree:

\Tree[. 1 [. [.i [. 2 3 ] 4 ] 5 ]]

Then the node marked $i$ will have
$l=1$, $x=2$, $y=1$
which gives
\[ L_i = M = \left[ \begin{matrix}
	0 & 0 & 0 & 0 & 0\\
	0 & -1 & 0 & 1 & 0\\
	0 & 0 & -1 & 1 & 0\\
	0 & 1 & 1 & -2 & 0\\
	0 & 0 & 0 & 0 & 0
\end{matrix} \right] \]

From this it becomes clear that all but $x+y$ of the eigenvalues will be zero,
and zero-rowsum brings this down to $x+y-1$ non-zero eigenvalues.

As a side note this means that the matrix of all of the eigenvalues will be
at least half zeroes, which simplifies a lot of calculations on this
matrix, especially once we find a way of permuting the eigenvalue matrix to be
upper triangular.

\subsection{Diagonalization}

given the simple $M_{ij}$ form above, it becomes possible to find eigenvectors
for these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
=
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
\left[\begin{matrix}
	0 & 0 & 0\\
	0 & -1 & 0\\
	0 & 0 & -3
\end{matrix}\right]
\end{equation*}

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
=
-1
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
\end{equation*}

In the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
In the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of $<1, -1, 0\ldots 0>$ will generalize to any $M_{ij}$ as above, as
long as $x \geq 2$, with an eigenvalue of -y.
Similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of $<0\ldots 0, -1, 1>$ with an eigenvalue of
-x.

Further still, we can get $x-1$ of the former and $y-1$ of the latter by moving
the -1 coordinate to any other row $\leq$ x, and the above argument would still
apply.

This gives $x-1$ repeated eigenvalues of $-y$, $y-1$ repeated eigenvalues of
$-x$, which when combined with the 0 eigenvalue shown, and the $n - (x + y)$
trivial $0$ eigenvalues coming from rows that are all zero, we get $n-1$ total
eigenvalues, meaning there is 1 more before we have a general solution.

This must correspond to the third eigenvector we get in the simple 3-tree case:

$
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
=
-3
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
$

If we guess that the general form is $<a, a\ldots a, b, b\ldots b>$ so that the
a repeats y times, and the b repeats x times, then upon application of M as
above, we get

\[ {[Mv]}_i = \begin{cases}
	-y*a + y*b & \text{if } i <= x\\
	x*a - x*b & \text{if } i > x
\end{cases} \]

Then if we suppose this vector $Mv$ equals $\lambda v$ then that gives two
equations:

\begin{equation}
	\lambda a = (yb - ay)
\end{equation}
\begin{equation*}
	\lambda b = (ax - xb)
\end{equation*}

Next we eliminate $\lambda$ and start to solve for $b$

\begin{equation*}
	(yb - ay)/a = (ax - xb)/b\\
\end{equation*}
\begin{equation*}
	b^2y - aby = a^2x - abx\\
\end{equation*}
\begin{equation*}
	b^2y + ab(x - y) - a^2x = 0
\end{equation*}
\begin{align*}
	r &= \frac{-a(x-y) \pm \sqrt{{a(x-y)}^2 + 4a^2xy}}{2y}\\
	  &= a\frac{y - x \pm (x + y)}{2y}\\
	  &= a or a\frac{-x}{y}
\end{align*}

$b = a$ corresponds to the zero-rowsum eigenvector from above, so it is not
new.
$b = a\frac{-x}{y}$ is new however, so set $a = y$, $b = -x$.

then our eigenvalue $\lambda$ can be derived from (1):
\begin{align*}
	\lambda &= \frac{y(b - a)}{a}
			&= -(x + y)
\end{align*}

This is our last eigenvalue, corresponding to the following eigenvector:

$\langle y, y\ldots y, -x, -x\ldots -x\rangle$

where $y$ is repeated $x$ times, and $-x$ is repeated $y$ times.

This means that all of our basis vectors are fully diagonalizable, as we have
$x+y$ nontrivial eigenvectors, plus $n-x+y$ empty rows, giving a total of $n$
eigenvectors.

\subsection{Simultaneous Diagonalization}

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices at once.

If we look at each matrix $L_i$, we can observe that the last eigenvalue in our
list above, $\lambda = -(x + y)$ is unique, and hence the eigenvector that
corresponds to it is uniquely determined by it. (modulo scaling)

Further in the matrix corresponding to the root of the tree, the $0$ eigenvalue
is also unique, so this node will actually give 2 uniquely determined
eigenvectors.

So in total we have determined $n$ eigenvectors!

Inspection of these shows that none of them are proportional to eachother,
since they all contain zeroes or negatives in different places to eachother.

Then we know that:
\begin{itemize}
	\item a set of linearly independent eigenvectors must exist,
	\item each of our eigenvalues is proportional to one of these vectors
	\item none of our eigenvalues are proportional to eachother
\end{itemize}
From this we can conclude that this must be a rescaling of some linearly
independent set of simultaneous eigenvectors, and hence is also a linearly
independent set of simultaneous eigenvectors.

If we label each eigenvector with the basis matrix that determined it, plus
$v_0$ as the zero-rowsum eigenvector, then the the eigenvalues of these
eigenvectors has a novel relationship with the structure of the binary tree:
\begin{itemize}
	\item ${L_i}{v_i} = -(x_i + y_i)v_i$ noting $v_i$ and $L_i$ come from the
		same node
	\item ${L_i}{v_j} = -{x_i}{v_j}$ if $j$ sits on the left subtree under $i$
	\item ${L_i}{v_j} = -{y_i}{v_j}$ if $j$ sits on the right subtree under $i$
	\item ${L_i}{v_j} = 0$ if $j$ sits outside of the subtree under $i$,
		or $j=0$
\end{itemize}

These results follow from constructing $v_j$ as a linear combination of the
eigenvectors described previously for $L_i$

So we have the exact value of our $n$ eigenvectors and eigenvalues.

\subsection{Ordering the Nodes}

If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular. (Once the zero-rowsum
column is removed)

One consequence of this is that the determinant will simply be

\[\prod_{i=1}^n d_i\]

This is simply the pre-order of the nodes, and so we have a closed form for the
determinant.

As an example of what all of the eigenvectors and eigenvalues look like in
matrix form, take two trees with 4 leaves:

\Tree[. [. 1 2 ] [. 3 4 ]]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 2 & 1 & 0\\
	1 & 2 & -1 & 0\\
	1 & -2 & 0 & 1\\
	1 & -2 & 0 & -1
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -2 & -2\\
	0 & 0 & -2 & 0\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]



\Tree[. [. [. 1 2 ] 3 ] 4 ]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 1 & 1 & 1\\
	1 & 1 & 1 & -1\\
	1 & 1 & -2 & 0\\
	1 & -3 & 0 & 0
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -1 & -1\\
	0 & 0 & -3 & -1\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]

\section{Deciding Fairness}

\subsection{Ubiquity in Eigenvalue Matrix}

The reference paper ``ubiquity of synonymity'' used an induction-like argument,
that if two trees are synonymous, then they sometimes also have an `exchange'
property, that adding branches around those trees will create further pairs of
synonymous trees.
%not really induction like now that i think about it

Then as tree size gets larger, the probability of containing a subtree with the
exchange property goes to 1, and hence so does the probability of another tree
existing with the same measurement.

In that case the measurement being used was the spectrum of a direct matrix
representation of the tree, but in our case one possible measurement is the
spectrum of the matrix of eigenvalues we have constructed.

This measurement for us immediately has the exchange property, as the spectrum
of our upper triangular matrices is simply the diagonal, and this diagonal is
simply the number of leaves in each subtree.

So then as long as there is a pair of trees with the same spectrum, we'll get
the same strong negative result for the spectrum.

\Tree[.    [. [. 1 2 ] [. 3 4 ]] [. 5   [. [. 6 [. 7 [. 8 9 ]]]]]]
\Tree[. [. [. [. 1 2 ] [. 3 4 ]]    5 ] [. [. 6 [. 7 [. 8 9 ]]]]]

As a result any measurement which is a symmetric function of the diagonal, such
as the determinant or the trace, will be at least as likely to coincide.

The two exceptions to this are, the trace of the eigenvalue matrix when not
triangular, and the spectrum of the eigenvalue matrix with ordering
information.

\subsection{Spectrum as Measure of Balance}

Interestingly, the trace will equal to the Sackin index of the tree, which is the sum of the path lengths from the root to each leaf of the tree.

The determinant seems quite unrelated to path lengths, but will still measure tree balance for similar reasons to the trace.

As an example the two trees of order four have a spectrum of $(4, 2, 2)$ and $(4, 3, 2)$ respectively.
The larger nodes in unbalanced trees can be expected to make both the trace and the determinant larger.

\subsection{Fairness of Eigenvalue Matrix}

It looks like both the eigenvector matrix, and the eigenvalue matrix, can be
used independent of eachother to reconstruct the tree structure.

The eigenvector matrix can be used to recover the leaf labels as well, whereas
the eigenvalue matrix can be used to recover the branch labels.

We know that the eigenvalue matrix is a fair representation of the tree with
internal nodes labelled, so long as we can reconstruct the tree and labels from
the eigenvalue matrix.

The eigenvalue matrix has 3 non-redundant sets of data in it:
\begin{itemize}
	\item The most extreme number in a row indicates the number of leaves under
		a node
	\item The column of that most extreme number indicates the eigenvector
		labelling
	\item Nonzero cells indicate that that column's node exists in the subtree
		of that row's node
\end{itemize}

So the first thing one could do is decompose the matrix into those 3 functions.

Define $W$ to be the matrix of eigenvalues, \emph{without the zero-rowsum
column}, and with negated (and hence positive) entries.

Define $I$ to be the set of integers $1$ thru $n-1$, i.e. $\mathds{Z} \cap [1,
n-1]$

\[\mathit{Size}(i) := \max\{W_{ij} | j \in I\}\]
\[\mathit{Col}: I \rightarrow I\]
\[\forall i \in I, W_{i \mathit{Col}(i)} = \mathit{Size}(i)\]
\[j \preccurlyeq i \Leftrightarrow W_{i \mathit{Col}(j)} > 0\]

$\mathit{Col}$ is well defined since we know $\mathit{Size}(i)$ is the unique
eigenvalue of $L_i$.

$\preccurlyeq$ is the original tree understood as a partial order, but with
leaves removed.

Simply reconstructing the tree from this partial order, and supplementing nodes
with leaves until it is a binary tree, should give the original tree.

On the other hand, one can use the $\mathit{Size}$ function, by taking the two
largest descendants as the children of each node, again supplemented with
leaves until each node has 2 children.

We know that this is in fact the original tree, as we've shown that the
eigenvalues of each matrix correspond to the structure of the tree.
% TODO show that the eigenvalues of each matrix correspond to the structure of
% the tree a bit more rigorously

\subsection{Fairness of Eigenvector Matrix}

The construction here is simple, if we use the eigenvector column indeces as
labels for the internal nodes, then the partial order is defined as:
\[j_1 \preccurlyeq j_2 \Leftrightarrow \forall i \in J, V_{ij_1} > 0
\Rightarrow V_{ij_2} > 0 \]

Then as before each node neads to be supplemented with any leaves, taking their
labels from the row indeces of $V$.

\section{loose notes}

\subsection{proof notes}

theres a bunch of things that could help some of the proofs above become more
clear.

First eigenvectors with non-zero eigenvalues have $v_{j} \neq 0 \Rightarrow$
leaf $j$ is a descendant of node $i$

In particular for the most extreme/unique eigenvalue the converse is also true.

Recovering tree structure from the vectors is then simple.
Recover the set of leaf descendants from above, then all if the descendants of
one node are also descendants of another node, the former is a descendant of
the latter.

Further we know that none of these can be proportional to eachother, or they
would belong to the same node, and hence be the same anyway.
The exception here is the zero-rowsum eigenvector, but we know none of the
constructed eigenvectors are proportional to that because its eigenvalue is
always 0.

Second the non-unique eigenspaces of the matrices correspond to tree structure
as well.
Leaves that dont descend from the node form an eigenspace on 0.
leaves that descend from the left child, restricted to have a sum of zero, form
An eigenspace on the size of the left subtree.
Likewise for the right child.

Then each of the eigenvectors in our set will sit in one of these three spaces
for all other nodes in the tree, which is very convenient.

This gives us the structural information we need to show that the eigenvalue
matrix is a fair representation of the original tree.

The fact that a binary tree can be reconstructed from the tree of its internal
nodes is probably a reference?

\subsection{minimum determinant}

each row of a balanced tree with $2^n$ leaves, is $2^i$ lots of $2^{n-i}$\ldots
right?

\subsection{Meeting 2}

the determinants seem to measure how balanced the trees are, from maximally
balanced: (expr?), to maximally unbalanced: n!

The original question still remains, how much could be removed from the matrix
without breaking injectivity. Could you reconstruct the tree from an ordered
spectrum?

\subsection{eig matrices}

why do the two matrices need leaf/branch info to recover the original? could
you get rid of it? if you cant is it impossible to use these matrices to
distinguish tree shapes?

well chosen matrix norms of either matrix might stifle the exchange property
matrix differences might be useful once the matrices have been sorted? but then
a tree will not have 0 distance from its own relabellings.

\subsection{eigenvalue polynomials}

the eigenvectors can be discovered through a polynomial process, by looking at
the product of each pair of basis matrices, as a linear combination of other
basis matrices

\subsection{hierarchies}

the eigenvector subset argument could be quite simple if described in terms of
hierarchies?

\subsection{Symmetries under relabelling}

The eig spaces of the basis matrices all have a sum of zero, except the all-1
eigenvector

corresponds exactly to the symmetries of the tree under the action of leaf
relabellings.

In particular each node is agnostic to relabellings of its left subtree, and of
its right subtree, so each node restricts the eigenspaces only with local
knowledge, until eventually the full structure of the tree has been
represented.

\subsection{Different kind of symmetric function}

When trying to summarize the eigenvalue matrix, we specifically want an
operation that is symmetric under row permutations, and under column
permutations, but not on arbitrary value permutations.

$f(X) = f(K_1XK_2)$

Then we can take further restrictions like ``must be homogeneous'' or whatever.

Under the wrong restrictions, we should get imanant polynomials out, but
steering clear of that might give something useful?

\end{document}
