\documentclass{report}
\usepackage{amsmath, dsfont, qtree}
\begin{document}

\section{Abstract}

We investigate representations of binary trees related to a particular matrix
algebra representation demonstrated previously.

[finish report first then finish abstract]

\section{Constructing Measurements of the Algebra}

\subsection{Background}

The paper ``Ubiquity of synonymity'' dealth with 3 different matrix
representations of trees: the adjacency matrix, laplacian matrix, and distance
matrix.
These representations are interesting, since relabellings of the tree result
in similar matrices, so their spectra will not change.
This means the spectra are themselves a representation of the tree, but the
paper shows that these representations are not fair, and in fact `approach'
triviality as the number of leaves increases.

A motivation indicated by the paper is to derive a distance function between
the structure of two trees.
Such a function would still have potential use if it was derived from an unfair
representation, but the result that representations become near trivial means
most trees would also have zero distance
As such while it would be nice to have a fair representation, it is crucial
that it is at least substantially non-trivial.

The Tas Phylo group has shown in another paper that trees can be represented as
an algebra of matrices, and that this algebra is commutative.
% algebra definition?
% proper references?

Similar to the previous paper, these matrices relate rows/columns to leaf
labels, and hence are similar to matrices associated with relabellings of the
tree.
Further, since the algebra is commutative, we have the property that if it
diagonalizable at all, then it is simultaneously diagonalizable.
Together these properties suggest that the spectra of matrices in this algebra
might be useful.

\subsection{Canonical Forms of Spectra}

If we assume that the matrices in the algebra are diagonalizable for now, (we
can show this later) then we can start to reason about the spectra of these
matrices.
With this assumption, it will also follow that all of the matrices in the
algebra are simultaneously diagonalizable, and hence that a set of eigenvectors
can be found common to the whole algebra.
We shall therefore refer to these as the Algebra's eigenvectors.

With this in place, we consider the spectra of the matrices in the algebra.
When representing each tree as a single matrix, comparing the spectra of these
matrices was a simple matter of comparing the multiplicity of each eigenvalue.
This could be thought of as either comparing the spectrum as a multiset, or as
a sorted sequence of values.
Then since the order of the eigenvectors doesn't matter, we can freely relabel
trees without changing their representation.
%     I keep changing my wording, I should try to stick to the original wording
% of the reference papers. "invariance under relabelling" etc.

We, on the other hand, are considering a multidimensional space of matrices,
and it might be nice to have this same property of invariance under
relabelling.
As such we might consider the set of sorted spectra of the whole algebra.

This representation seems to be useful, but once we consider the simultaneous
diagonalization of the algebra, we see a missed opportunity to generate a
homomorphism from the algebra to the vector space $\mathds{C}^n$.
That is, by taking the algebra's eigenvectors under some ordering, and
generating the spectra of matrices corresponding to these eigenvectors, we will
get the property that the spectrum of a linear combination of matrices is the
same linear combination of the individual spectra.
This means the ``spectrum'' map is a homomorphism, and so its image will be a
vector space.

This will not apply to the sorted image, e.g. $<1, 1> - <0, 1> = <1, 0>$
	% <tuple> notation?
So as an alternative, we could consider this vector space, and look for
properties that are invariant under relabelling of the original tree, and
invariant under different choices of eigenvector.
This is the same as finding properties that are invariant under permutation of
the axes of the vector space.

% note that it might look like we have strayed from any sense of distance formula
% measurements later on will have potential for distance formulae

\subsection{Subspace Triviality}

When looking at this vector space, the most obvious measurement we could apply
to it is its dimension.
It will turn out that its dimension is always the same as the algebra, which
seems odd when the algebra seems to contain matrices that are just relabellings
of eachother.
As an example every cherry in a tree will correspond to a matrix equivalent to
the following:

\[
	\left[ \begin{matrix}
		-1 & 1\\
		1 & -1
	\end{matrix} \right]
\]

The exact matrices are simply the above with added rows and columns of zero.
The spectrum of any matrix in this form should be the same as any other, and in
the sorted-list representation they will be, but our vector space will
distinguish these, in the same way that $<0, 1>$ and $<1, 0>$ are linearly
independent.

This means that symmetries in the tree might have created interesting
redundancies in the set of sorted spectra, but our vector space will not
reflect these symmetries in the same way.

In particular if we say that the number of leaves on the original tree is $n$,
then our algebra will be generated from $n-1$ internal nodes, and will be a set
of $n \times n$ matrices.

This means that the vector space will be an $n-1$ dimensional subspace of
$\mathds{C}^n$.

Unfortunately since the matrices all have zero row-sum, that corresponds to an
eigenvector of all 1s, and an eigenvalue of zero, which means the subspace will
simply be $\mathds{C}^{n-1}$ extended by a zero, regardless of the tree in
question.

Not only is the vector space not a fair representation of the binary tree, it
is a trivial representation.

\subsection{Parameter-Free Representations}

While it is not useful to map the whole algebra to $\mathds{C}^n$, if we just
look at the basis it turns out to be more useful.
% addressing the fact that other bases exist than the one the algebra was
% defined with? once we're talking about determinants this can be dealt with
% specifically, but the trace and spectrum of the eigenvalue matrix are heavily
% dependent on the basis chosen
As such we shall consider the basis that was used to define the algebra.

If the original tree has $n$ leaves, then we have
\begin{itemize}
	\item $n-1$ internal nodes
	\item $n-1$ basis matrices
	\item $n$ common eigenvectors
	\item $(n-1) \times n$ eigenvalues
\end{itemize}

This could be summarized in a pair of matrices:
\begin{itemize}
	\item An $n \times n$ matrix of eigenvectors, and
	\item an $(n-1) \times n$ matrix of eigenvalues.
\end{itemize}

This second matrix will always have a column of zeroes in it, corresponding to
the eigenvector of all 1s, since the algebra has zero row-sum.
As such we do not lose any information by removing this column, and getting a
square $(n-1) \times (n-1)$ matrix.
We could do the same for the eigenvector and get an $n \times (n-1)$ matrix,
but obviously this has the opposite effect.

These matrices are not invariant under relabelling:
\begin{itemize}
	\item relabelling leaves permutes the rows of the eigenvector matrix
	\item relabelling internal nodes permutes the rows of the eigenvalue matrix
	\item relabelling eigenvectors permutes the columns of both matrices.
\end{itemize}

It seems like either of these matrices on their own would be enough to
reconstruct the algebra, and hence the tree, so on their own they are
interesting representations of a tree.
Further still, measurements that are invariant under the row/column
permutations above have potential for distinguishing tree-shapes as was
originally desired.

One clear possibility is to take the determinant of either of these matrices,
and remove the effect of row/column permutations with an absolute value.
For now we shall derive the exact value of both of the matrices.

% not included currently, could be a useful analogy:
% Geometrically we can consider this to be the volume spanned by the spectrum vectors.

\section{Simultaneous Diagonalization}

%\subsection{basis variance}
%
%If $detsubst(L*) =$ the determinant as described, but with respect to basis
%$L*, L_2, L_3\ldots L_n-1$, so that $detsubst(L_1)$ is exactly the
%determinant as described.
%
%Then $detsubst(L_1 + L_2)$ expanded on the first column seems to give the same
%expression as $detsubst(L_1) + detsubst(L_2)$ expanded on their first columns.
%
%But $detsubst(L_2)$ is the determinant of a singular matrix:
%\[[spec(L_2), spec(L_2), spec(L_3)\ldots]\]
%so equals zero.
%
%The same doesn't seem to apply to $detsubst(L_1 + L_1)$ though, so we might
%need to normalize the $L_i$ matrices somehow?

When working towards a simultaneous diagonalization of the algebra, the first
thing we need is to show that the matrices can be diagonalized at all.
In showing this we expect to get the eigenvalues as well.


\subsection{Simple Matrix Format}

For simplicity we will assume that the tree leaves have been labelled ``from
left to right'', i.e.\ such that at any internal node, its left descendants are
less than its right descendants.

Formally, that there is a total order of the tree such that the ``left'' child
of any node (being the minimum of the two children) has descendents that are
all less than the ``right'' child of that node.

Other labellings will simply permute the coordinates of the eigenvectors we
derive here.

Then we can see that the matrix that corresponds to a node with $x$ left
descendents, $y$ right descendents, and minimum descendent $l+1$. (so that
there are $l$ leaves to the left of the node's minimum descendent)

\[ M_{ij} = \begin{cases}
-y & l < i = j \leq l + x\\
-x & l + x < i = j \leq l + x + y\\
1 & l < i \leq l + x < j \leq l + x + y\\
1 & l < j \leq l + x < i \leq l + x + y\\
0 & otherwise
\end{cases} \]

So for example if we take the tree:

\Tree[. 1 [. [.i [. 2 3 ] 4 ] 5 ]]

Then the node marked $i$ will have
$l=1$, $x=2$, $y=1$
which gives
\[ L_i = M = \left[ \begin{matrix}
	0 & 0 & 0 & 0 & 0\\
	0 & -1 & 0 & 1 & 0\\
	0 & 0 & -1 & 1 & 0\\
	0 & 1 & 1 & -2 & 0\\
	0 & 0 & 0 & 0 & 0
\end{matrix} \right] \]

From this it becomes clear that all but $x+y$ of the eigenvalues will be zero,
and zero-rowsum brings this down to $x+y-1$ non-zero eigenvalues.

As a side note this means that the matrix of all of the eigenvalues will be
at least half zeroes, which simplifies a lot of calculations on this
matrix, especially once we find a way of permuting the eigenvalue matrix to be
upper triangular.

\subsection{Diagonalization}

given the simple $M_{ij}$ form above, it becomes possible to find eigenvectors
for these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
=
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
\left[\begin{matrix}
	0 & 0 & 0\\
	0 & -1 & 0\\
	0 & 0 & -3
\end{matrix}\right]
\end{equation*}

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
=
-1
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
\end{equation*}

In the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
In the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of $<1, -1, 0\ldots 0>$ will generalize to any $M_{ij}$ as above, as
long as $x \geq 2$, with an eigenvalue of -y.
Similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of $<0\ldots 0, -1, 1>$ with an eigenvalue of
-x.

Further still, we can get $x-1$ of the former and $y-1$ of the latter by moving
the -1 coordinate to any other row $\leq$ x, and the above argument would still
apply.

This gives $x-1$ repeated eigenvalues of $-y$, $y-1$ repeated eigenvalues of $-x$,
which when combined with the 0 eigenvalue shown, and the $n - (x + y)$ trivial $0$
eigenvalues coming from rows that are all zero, we get $n-1$ total eigenvalues,
meaning there is 1 more before we have a general solution.

This must correspond to the third eigenvector we get in the simple 3-tree case:

$
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
=
-3
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
$

If we guess that the general form is $<a, a\ldots a, b, b\ldots b>$ so that the
a repeats y times, and the b repeats x times, then upon application of M as
above, we get

\[ {[Mv]}_i = \begin{cases}
	-y*a + y*b & \text{if } i <= x\\
	x*a - x*b & \text{if } i > x
\end{cases} \]

Then if we suppose this vector $Mv$ equals $\lambda v$ then that gives two
equations:

\begin{equation}
	\lambda a = (yb - ay)
\end{equation}
\begin{equation*}
	\lambda b = (ax - xb)
\end{equation*}

Next we eliminate $\lambda$ and start to solve for $b$

\begin{equation*}
	(yb - ay)/a = (ax - xb)/b\\
\end{equation*}
\begin{equation*}
	b^2y - aby = a^2x - abx\\
\end{equation*}
\begin{equation*}
	b^2y + ab(x - y) - a^2x = 0
\end{equation*}
\begin{align*}
	r &= \frac{-a(x-y) \pm \sqrt{{a(x-y)}^2 + 4a^2xy}}{2y}\\
	  &= a\frac{y - x \pm (x + y)}{2y}\\
	  &= a or a\frac{-x}{y}
\end{align*}

$b = a$ corresponds to the zero-rowsum eigenvector from above, so it is not
new.
$b = a\frac{-x}{y}$ is new however, so set $a = y$, $b = -x$.

then our eigenvalue $\lambda$ can be derived from (1):
\begin{align*}
	\lambda &= \frac{y(b - a)}{a}
			&= -(x + y)
\end{align*}

This is our last eigenvalue, corresponding to the following eigenvector:

$\langle y, y\ldots y, -x, -x\ldots -x\rangle$

where $y$ is repeated $x$ times, and $-x$ is repeated $y$ times.

This means that all of our basis vectors are fully diagonalizable, as we have
$x+y$ nontrivial eigenvectors, plus $n-x+y$ empty rows, giving a total of $n$
eigenvectors.

\subsection{Simultaneous Diagonalization}

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices at once.

If we look at each matrix $L_i$, we can observe that the last eigenvalue in our
list above, $\lambda = -(x + y)$ is unique, and hence the eigenvector that
corresponds to it is uniquely determined by it. (modulo scaling)

Further in the matrix corresponding to the root of the tree, the $0$ eigenvalue
is also unique, so this node will actually give 2 uniquely determined
eigenvectors.

So in total we have determined $n$ eigenvectors!

Inspection of these shows that none of them are proportional to eachother,
since they all contain zeroes or negatives in different places to eachother.

Then we know that:
\begin{itemize}
	\item a set of linearly independent eigenvectors must exist,
	\item each of our eigenvalues is proportional to one of these vectors
	\item none of our eigenvalues are proportional to eachother
\end{itemize}
From this we can conclude that this must be a rescaling of some linearly
independent set of simultaneous eigenvectors, and hence is also a linearly
independent set of simultaneous eigenvectors.

If we label each eigenvector with the basis matrix that determined it, plus
$v_0$ as the zero-rowsum eigenvector, then the the eigenvalues of these
eigenvectors has a novel relationship with the structure of the binary tree:
\begin{itemize}
	\item $L_iv_i = -(x_i + y_i)v_i$ noting $v_i$ and $L_i$ come from the same
		node
	\item $L_iv_j = -x_iv_j$ if $j$ sits on the left subtree under $i$
	\item $L_iv_j = -y_iv_j$ if $j$ sits on the right subtree under $i$
	\item $L_iv_j = 0$ if $j$ sits outside of the subtree under $i$, or $j=0$
\end{itemize}

These results follow from constructing $v_j$ as a linear combination of the
eigenvectors described previously for $L_i$

So we have the exact value of our $n$ eigenvectors and eigenvalues.

\subsection{Ordering the Nodes}

If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular. (Once the zero-rowsum
column is removed)

One consequence of this is that the determinant will simply be

\[\prod_{i=1}^n d_i\]

This is simply the pre-order of the nodes, and so we have a closed form for the
determinant.

As an example of what all of the eigenvectors and eigenvalues look like in
matrix form, take two trees with 4 leaves:

\Tree[. [. 1 2 ] [. 3 4 ]]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 2 & 1 & 0\\
	1 & 2 & -1 & 0\\
	1 & -2 & 0 & 1\\
	1 & -2 & 0 & -1
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -2 & -2\\
	0 & 0 & -2 & 0\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]



\Tree[. [. [. 1 2 ] 3 ] 4 ]

\nopagebreak[4]

\[ \text{eigenvectors} = \left[ \begin{matrix}
	1 & 1 & 1 & 1\\
	1 & 1 & 1 & -1\\
	1 & 1 & -2 & 0\\
	1 & -3 & 0 & 0
\end{matrix} \right] \]

\nopagebreak[4]

\[ \text{eigenvalues} = \left[ \begin{matrix}
	0 & -4 & -1 & -1\\
	0 & 0 & -3 & -1\\
	0 & 0 & 0 & -2
\end{matrix} \right] \]

\section{injectivity}

The reference paper ``ubiquity of synonymity'' used an induction-like argument,
that if two trees are synonymous, then adding branches around those trees will
create further pairs of synonymous trees, which looks almost trivial to show
for this spectral determinant.

As such if there is any synonymity then as the paper showed, it will become
ubiquitous for large enough n.

It seems unlikely that there would be \emph{no} synonymity, which is the only way
for there to be \emph{anything less than ubiquitous} synonymity.


\subsection{minimum determinant}

each row of a balanced tree with $2^n$ leaves, is $2^i$ lots of $2^(n-i)$\ldots right?

\section{Meeting 2}


looking at spectra generated for trees of size 4 and 5 confirmed the result
that spectral matrices are upper triangular, and that diagonals are the
negative of the size of the subtree corresponding to that node

the determinants seem to measure how balanced the trees are, from maximally
balanced: (expr?), to maximally unbalanced: n!


However since the matrices are triangular, it does look like the determinants
coincide at 8 leaves
another permutation-independant measure is the trace, which coincides at 6
leaves.

This discussion splits two ways, first is the fact that the trace does depend
on how eigenvectors are associated with basis matrices, changing the relation
creates different trace values.
Second is the fact that the trace and the determinant are symmetric functions
of the diagonal, i.e.\ of the eigenvalues \emph{of the spectral matrix}.

It turns out that there is a pair of trees with 9 leaves that have their whole
diagonal the same! (modulo ordering)
this means that any function which is symmetric (and hence doesn't depend on
relabelling) will be non-injective as well.

On the other hand the matrix as a whole appears to be injective, so it is worth
asking how much could be removed from the matrix without breaking injectivity.
Could you reconstruct the tree from an ordered spectrum?


\end{document}
