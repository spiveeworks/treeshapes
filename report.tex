\documentclass{report}
\usepackage{amsmath, dsfont, qtree}
\begin{document}

\section{Background}

The paper ``Ubiquity of synonymity'' dealth with 3 different matrix
representations of trees: the adjacency matrix, laplacian matrix, and distance
matrix.
These representations are interesting, since relabellings of the tree result
in similar matrices, so their spectra will not change.
This means the spectra are themselves a representation of the tree, but the
paper shows that these representations are not fair, and in fact `approach'
triviality as the number of leaves increases.

A motivation indicated by the paper is to derive a distance function between
the structure of two trees.
Such a function would still have potential use if it was derived from an unfair
representation, but the result that representations become near trivial means
most trees would also have zero distance
As such while it would be nice to have a fair representation, it is crucial
that it is at least substantially non-trivial.

The Tas Phylo group has shown in another paper that trees can be represented as
an algebra of matrices, and that this algebra is commutative.
% algebra definition?

Similar to the previous paper, these matrices relate rows/columns to leaf
labels, and hence are similar to matrices associated with relabellings of the
tree.
Further, since the algebra is commutative, we have the property that if it
diagonalizable at all, then it is simultaneously diagonalizable.
Together these properties suggest that the spectra of matrices in this algebra
might be useful.

\section{Canonical Forms of Spectra}

(Assuming that the algebra is diagonalizable at all)

One consideration with spectral representations, is how to order the
eigenvalues, and their corresponding eigenvectors.  When saying that two
matrices have the same eigenvalues, what one means is that there is an ordering
of one matrices eigenvalues that correspond identically with the eigenvalues of
the other matrix. This will account for the multiplicity of each eigenvalue.
This suggests 3 ways of canonicalizing the spectra, one is to check the
necessary condition that each eigenvalue in the first has at least one
corresponding eigenvalue in the second, by representing the spectra as a set
of eigenvalues. This shall be immediately discarded, since it could so easily
compromise the injectivity of the representation, since
$\{1, 1, 2\} = \{1, 2, 2\}$

The other two ways are to sort the list (choose a canonical rearrangement) and
equivalently to use a multiset.

Sorting the list will definitely work for the single matrix representations
above, but for the algebra representation, this would compromise the
motivating linearity property that emerges from the simultaneous
diagonalizability of the algebras.
In particular $<1, 1> - <0, 1> = <1, 0>$ which isn't sorted.
Sorting the result will restore the equality, but at this point the set of
spectra is looking a lot less convenient, though it could still be of interest.

Without sorting, the image of the algebra forms a vector space, whose dimension
could range from 1 to n-1 where n is the number of leaves on the trees in
question.
Since the matrices are simultaneously diagonalizable, we can pick a single set
of eigenvectors in advance, and use this to compute all the spectra, resulting
in a single vector space, however different permutations of these eigenvectors
will be valid and will permute the coordinates in $\mathds{C}^n$.

It is unclear how to apply this to get a distance formula..

so the question of identifying similar spectra becomes a question of similar
vector spaces generated from spectra\ldots
from this perspective it is clear that sorting no longer works.

In any case then the important stuff:
\begin{itemize}
	\item subspace has at most n-1 dimensions
	\item subspace could have less if there is degeneracy
	\item in particular if two subtrees have the same shape then their roots
		will have the same corresponding matrices.
	\item further all algebras of a given n will have one consistent
		eigenvalue of 0,
	\item this means that if there is no degeneracy the image will always be
		$\mathds{C}^{n-1}$
	\item this means the representation is definitely not injective, however
		the dimension will still be a measure of degeneracy
	\item also degenerate trees may have distinct images?
\end{itemize}

\section{eigenvalues}

\subsection{eig matrix}

For an n leaf tree, we have $\dim(A)=n-1$ for the tree algebra.

Let $s(L_i)$ be the ith (ordered/include multiplicity) spectrum of the basis
matrix $L_i$. So $s(L_i)$ is an n-vector.

Each $L_i$ has a 0 as an eigenvalue corresponding to the col vector of all 1s.
Remove this from the spectrum vectors.

Thus we have (n-1) (n-1)-vectors.

Form a square matrix out of these and take the absolute value of its
determinant.

Then this value will only depend on the tree shape, since the determinant simply acquires +/- under permutation of rows/columns.

(Geometrically we can consider this to be the volume spanned by the spectrum vectors.)



\subsection{basis variance}

If $detsubst(L*) =$ the determinant as described, but with respect to basis
$L*, L_2, L_3\ldots L_n-1$, so that $detsubst(L_1)$ is exactly the
determinant as described.

Then $detsubst(L_1 + L_2)$ expanded on the first column seems to give the same
expression as $detsubst(L_1) + detsubst(L_2)$ expanded on their first columns.

But $detsubst(L_2)$ is the determinant of a singular matrix:
\[[spec(L_2), spec(L_2), spec(L_3)\ldots]\]
so equals zero.

The same doesn't seem to apply to $detsubst(L_1 + L_1)$ though, so we might
need to normalize the $L_i$ matrices somehow?



\subsection{degenerecy and eig matrix}

If two subtrees of the tree have the same spectrum (e.g.\ if they are the same)
then the matrix will have a repeated row, and hence will not be invertible, and
hence will have a determinent of zero, however this is specifically the case
where the vector space generated by the spectra is no longer trivial.

\subsection{symmetry and degeneracy}

when taking the span or determinent of the spectrum vectors, we see that
repeated spectra give a trivial span of $\mathds{C}^{n-1}$, and a determinent of 0

As such we care about whether a tree has degeneracy or not, and it would seem
that a tree with symmetry would be degenerate, e.g.:

\Tree[.a [.b 1 2 ] [.c 3 4 ]]

it looks like the internal nodes $b$ and $c$ have the same spectrum, being
relabellings of eachother, but in fact they do not, since we are looking at
the spectra generated by \emph{a fixed sequence of eigenvectors}, and hence
relabelling the leaves results in the eigenvectors being permuted, which is
undone only by permuting the eigenvalues, giving a different spectrum.

$A = PBP', B = VLV'$

$A = (PV)L(PV)'$

so $A v_p(i) = l_i v_p(i)$



\subsection{entropy and computation}

The matrix representations simply correspond $\{i_l\} \times \{i_r\}$
the leaf descendents of the left child of the node,
multiplied by the leaf descendents of the right child of the node
if we consider these matrices invariant under relabelling (which they aren't
quite) then it looks like this is just representing the size of each child
tree, and we could canonicalize as such:

any subtree's matrix is similar by permutation to a matrix of the form:

\[ M_{ij} = \begin{cases}
-y & i = j \leq x\\
-x & x < i = j \leq x + y\\
1 & i \leq x < j \leq x + y\\
1 & j \leq x < i \leq x + y\\
0 & otherwise
\end{cases}
\]

where x is the number of leaves on the left of the subtree's root,
and y is the number of leaves on the right
$x + y = n_i <= n$

this could be very useful computationally, especially since these matrices can
be extended or retracted by the rows and columns of zero

such an extension simply adds a zero eigenvalue corresponding to
$[0\ldots 0, 1]$
and a zero coordinate to each existing eigenvector without changing the
eigenvalue

this means that each spectrum will only have $x + y - 1$ non-zero eigenvalues,
so the matrix of eigenvalues will be about half zeroes

in fact for the case of the balanced 4-tree, the nodes and eigenvectors can be
rearranged so that the matrix has the form:

\begin{equation*}
\left[ \begin{matrix}
a & 0 & b\\
0 & c & d\\
0 & 0 & e
\end{matrix} \right]
\end{equation*}

which will clearly have a determinant of $ace$

it is not at all clear whether these upper triangular forms exist for all
trees, since the eigenvectors don't clearly correspond to leaves of the tree,
(if they did we might want to order the trees from smallest to largest, or
order the leaves of the tree by the number of subtrees they sit inside)
in fact eigenvectors in the null space may be an arbitrary basis of the null
space, however computing the eigenvalue matrices of more trees may provide
insight



\subsection{closed form eigenvectors}

given the simple $M_{ij}$ form above, it becomes possible to find eigenvectors for
these algebras in general.

If we take the root of the 3-tree for example, we get the following
eigenvectors:

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
=
\left[\begin{matrix}
	1 & 1 & 1\\
	1 & -1 & 1\\
	1 & 0 & -2
\end{matrix}\right]
\left[\begin{matrix}
	0 & 0 & 0\\
	0 & -1 & 0\\
	0 & 0 & -3
\end{matrix}\right]
\end{equation*}

the first eigenvector is trivial to understand, having an eigenvalue of 0 it
simply says that the row-sum of our matrix is 0, an intended feature of its
construction.

The second and third are more interesting, and can be understood by how they
act in the first two rows, (where i <= x) vs the last row (where x < i <= x+y)

\begin{equation*}
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
=
-1
\left[\begin{matrix}
	1\\
	-1\\
	0
\end{matrix}\right]
\end{equation*}

In the first two rows the eigenvalue directly appears in the matrix, and is the
only term that doesn't become a zero in the series.
In the last row the repeated 1s sum the coordinates of the matrix, which give
zero.

This form of $<1, -1, 0\ldots 0>$ will generalize to any $M_{ij}$ as above, as long as
$x \geq 2$, with an eigenvalue of -y
similarly if y >= 2 we could reverse the labels temporarily and use the same
argument, we get an eigenvector of $<0\ldots 0, -1, 1>$ with an eigenvalue of -x
further still we can get $x-1$ of the former and $y-1$ of the latter by moving the
-1 coordinate to any other row $\leq$ x, and the above argument would still apply.

This gives $x-1$ repeated eigenvalues of $-y$, $y-1$ repeated eigenvalues of $-x$,
which when combined with the 0 eigenvalue shown, and the $n - (x + y)$ trivial $0$
eigenvalues coming from rows that are all zero, we get $n-1$ total eigenvalues,
meaning there is 1 more.

This must be the third eigenvector we get in the simple 3-tree case:

$
\left[\begin{matrix}
	-1 & 0 & 1\\
	0 & -1 & 1\\
	1 & 1 & -2
\end{matrix}\right]
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
=
-3
\left[\begin{matrix}
	1\\
	1\\
	-2
\end{matrix}\right]
$

it's hard to understand exactly why this is an eigenvector, but if we guess
that the general form is $<a, a\ldots a, b, b\ldots b>$ so that the a repeats y
times, and the b repeats x times, then upon application of M as above, we get

if $i <= x$
then ${(Mv)}_{ij} = -y*a + y*b$
if $i > x$
then ${(Mv)}_{ij} = x*a - x*b$

then if we suppose that these equal l*a and l*b respectively, we get:

\begin{equation*}
	(yb - ay)/a = (ax - xb)/b\\
\end{equation*}
\begin{equation*}
	b^2y - aby = a^2x - abx\\
\end{equation*}
\begin{equation*}
	b^2y + ab(x - y) - a^2x = 0
\end{equation*}
let $r = \frac{b}{a}$
\begin{align*}
	r &= \frac{-(x-y) \pm \sqrt{{(x-y)}^2 + 4xy}}{2y}\\
	  &= \frac{y - x \pm (x + y)}{2y}\\
	  &= 1 or \frac{-x}{y}
\end{align*}

$r = 1$ corresponds to our common eigenvector from above, so it is not new.
$r = \frac{-x}{y}$ is new however, so set $b = -x$, $a = y$,

then our eigenvalue l:
\begin{align*}
	l &= \frac{y(b - a)}{a}
	  &= -(x + y)
\end{align*}

This is our last eigenvalue, corresponding to the following eigenvector:

$\langle y, y\ldots y, -x, -x\ldots -x\rangle$

where y is repeated x times, and x is repeated y times.


\subsection{Simultaneous Diagonalization}

The matrices that we are looking at form a commutative algebra, which means
there is some set of n linearly independent vectors that are eigenvectors for
all n-1 of our basis matrices.

If we look at each matrix $L_i$, we can observe that the last eigenvalue in our
list above, $\lambda = -(x + y)$ is unique, and hence the eigenvector that corresponds
to it is uniquely determined by it. (modulo scaling)

Since each matrix will give a different, uniquely determined vector for its
most unique eigenvalue, we have $n-1$ eigenvectors, which are also distinct from
our other uniquely determined vector $<1, 1\ldots 1>$.

So we know that that all of our eigenvectors are determined, and can use this
set of eigenvectors for diagonalization of any basis matrix.


If we number these vectors $v_i$, and set $v_0$ to be $<1, 1\ldots 1>$, then we can
understand which eigenvalues will correspond to different eigenvectors.

For $L_i$, $v_i$ will by definition give $L_i v_i = -(x+y) v_i$

for nodes descending from the left child of i, $L_i v_l = -y v_l$
and similarly on the right, $L_i v_r = -x v_r$
this can be seen both by the quantities of nodes, and more rigorously by the
fact that each of these $v_l$ is a linear combination of vectors constructed
above, with eigenvalue -y, and similarly for $v_r$ and $-x$

for nodes above and anywhere else, the nontrivial part of $L_i v_j$ will be
entirely within one of the four consistent sections of $v_j$, so $v_j$ might as
well be a multiple of $v_0$, giving $L_i v_j = 0 v_j$

So we know all n of our eigenvalues, and all of their corresponding
eigenvectors.


\subsection{Returning to the determinant}


If we can order these nodes so that their children always come after them, then
the matrix of eigenvalues will be upper triangular, and so its determinant will
simply be

\[\prod_{i=1}^n d_i\]

this is simply the pre-order of the nodes, and so we have a closed form of the
determinant.

The reference paper ``ubiquity of synonymity'' used an induction-like argument,
that if two trees are synonymous, then adding branches around those trees will
create further pairs of synonymous trees, which looks almost trivial to show
for this spectral determinant.

As such if there is any synonymity then as the paper showed, it will become
ubiquitous for large enough n.

It seems unlikely that there would be \emph{no} synonymity, which is the only way
for there to be \emph{anything less than ubiquitous} synonymity.


\subsection{minimum determinant}

each row of a balanced tree with $2^n$ leaves, is $2^i$ lots of $2^(n-i)$\ldots right?

\section{Meeting 2}


looking at spectra generated for trees of size 4 and 5 confirmed the result
that spectral matrices are upper triangular, and that diagonals are the
negative of the size of the subtree corresponding to that node

the determinants seem to measure how balanced the trees are, from maximally
balanced: (expr?), to maximally unbalanced: n!


However since the matrices are triangular, it does look like the determinants
coincide at 8 leaves
another permutation-independant measure is the trace, which coincides at 6
leaves.

This discussion splits two ways, first is the fact that the trace does depend
on how eigenvectors are associated with basis matrices, changing the relation
creates different trace values.
Second is the fact that the trace and the determinant are symmetric functions
of the diagonal, i.e.\ of the eigenvalues \emph{of the spectral matrix}.

It turns out that there is a pair of trees with 9 leaves that have their whole
diagonal the same! (modulo ordering)
this means that any function which is symmetric (and hence doesn't depend on
relabelling) will be non-injective as well.

On the other hand the matrix as a whole appears to be injective, so it is worth
asking how much could be removed from the matrix without breaking injectivity.
Could you reconstruct the tree from an ordered spectrum?


\end{document}
